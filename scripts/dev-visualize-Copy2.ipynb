{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util.util import load_opt\n",
    "\n",
    "def show(img, show=True):\n",
    "    npimg = img.numpy()\n",
    "    npimg = ((npimg / 2.0) + 0.5) * 255.0\n",
    "    npimg = npimg.astype(np.uint8)\n",
    "    if show:\n",
    "        plt.figure(figsize = (40,40))\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    return Image.fromarray(npimg.transpose((1,2,0)))\n",
    "\n",
    "import util.util as util\n",
    "from models import networks\n",
    "from options.test_options import TestOptions\n",
    "\n",
    "def load_network_with_path(network, network_label, epoch_label='latest', path='./checkpoints/danbooru_celeba_cyclegan/'):\n",
    "    save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "    save_path = os.path.join(path, save_filename)\n",
    "    network.load_state_dict(torch.load(save_path))\n",
    "\n",
    "def get_trans(crop=None, size=None):\n",
    "    transform_list_show = []\n",
    "    transform_list = []\n",
    "    if crop is not None:\n",
    "        opt.loadSize = crop\n",
    "    if size is not None:\n",
    "        opt.fineSize = size\n",
    "    osize = [opt.loadSize, opt.loadSize]\n",
    "    transform_list_show += [transforms.Scale(osize, Image.BICUBIC),\n",
    "                       transforms.RandomCrop(opt.fineSize),\n",
    "                       transforms.ToTensor()]\n",
    "    img_show = transforms.Compose(transform_list_show)\n",
    "    transform_list = transform_list_show + [transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))]\n",
    "    img_pre = transforms.Compose(transform_list)\n",
    "    return img_show, img_pre    \n",
    "\n",
    "def get_idtrans(crop=None, size=None):\n",
    "    transform_list_show = []\n",
    "    transform_list = []\n",
    "    transform_list_show += [transforms.ToTensor()]\n",
    "    img_show = transforms.Compose(transform_list_show)\n",
    "    transform_list = transform_list_show + [transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))]\n",
    "    img_pre = transforms.Compose(transform_list)\n",
    "    return img_show, img_pre   \n",
    "\n",
    "def to_input(img, trans, size):\n",
    "    img = trans(img)\n",
    "    input_ = img.view(-1, 3, size, size)\n",
    "    real = Variable(input_.cuda())\n",
    "    return real\n",
    "\n",
    "def test_img(img, net, trans, size, eval_mode=True, bn_eval=False, drop_eval=True):\n",
    "    img = trans(img)\n",
    "    #input_ = img.view(-1, 3, size, size)\n",
    "    input_ = img.unsqueeze(0)\n",
    "    real = Variable(input_.cuda())\n",
    "    set_eval(net, bn_eval, drop_eval)\n",
    "    fake = net.forward(real)\n",
    "    return fake\n",
    "\n",
    "def get_stack_nets(path, which_epoch = 'latest'):\n",
    "    gpu_ids = [0]\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    opt = load_opt(path)\n",
    "    netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, False, opt.init_type, \n",
    "                            gpu_ids, n_upsampling=3, opt=opt)\n",
    "    netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, False, opt.init_type, \n",
    "                            gpu_ids, n_upsampling=3, opt=opt)\n",
    "    netG_A_pre = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, False, opt.init_type, \n",
    "                            gpu_ids, n_downsampling=3, opt=opt)\n",
    "    netG_B_pre = networks.define_G(opt.output_nc, opt.input_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, False, opt.init_type, \n",
    "                            gpu_ids, n_downsampling=3, opt=opt)\n",
    "\n",
    "    load_network_with_path(netG_A,  'G_A', which_epoch, path)\n",
    "    load_network_with_path(netG_B,  'G_B', which_epoch, path)\n",
    "    load_network_with_path(netG_A_pre,  'G_A_pre', which_epoch, path)\n",
    "    load_network_with_path(netG_B_pre,  'G_B_pre', which_epoch, path)\n",
    "        \n",
    "    netG_A.cuda()\n",
    "    netG_B.cuda()\n",
    "    netG_A_pre.cuda()\n",
    "    netG_B_pre.cuda()\n",
    "    \n",
    "    return {'A':netG_A, 'B':netG_B, 'A_pre':netG_A_pre, 'B_pre':netG_B_pre}\n",
    "\n",
    "def get_stack_nets(path, which_epoch = 'latest', skip=True):\n",
    "    gpu_ids = [0]\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    opt = load_opt(path)\n",
    "    opt.max_ngf = 256\n",
    "    netG_A = networks.define_G(opt.input_nc if not skip else opt.input_nc * 2, opt.output_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, \n",
    "                            gpu_ids, n_upsampling=opt.n_upsample, opt=opt)\n",
    "    netG_B = networks.define_G(opt.input_nc if not skip else opt.input_nc * 2, opt.input_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, \n",
    "                            gpu_ids, n_upsampling=opt.n_upsample, opt=opt)\n",
    "    if 'pre_path' in vars(opt).keys():\n",
    "        opt_pre = load_opt(opt.pre_path)\n",
    "        netG_A_pre = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                                opt_pre.ngf, opt_pre.which_model_netG, opt_pre.norm, not opt_pre.no_dropout, opt_pre.init_type, \n",
    "                                gpu_ids, n_downsampling=3, opt=opt_pre)\n",
    "        netG_B_pre = networks.define_G(opt.output_nc, opt.input_nc,\n",
    "                                opt_pre.ngf, opt.which_model_netG, opt_pre.norm, not opt_pre.no_dropout, opt_pre.init_type, \n",
    "                                gpu_ids, n_downsampling=3, opt=opt_pre)\n",
    "    else:\n",
    "        opt.pre_path = ''\n",
    "        netG_A_pre = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, False, opt.init_type, \n",
    "                            gpu_ids, n_downsampling=3, opt=opt)\n",
    "        netG_B_pre = networks.define_G(opt.output_nc, opt.input_nc,\n",
    "                            opt.ngf, opt.which_model_netG, opt.norm, False, opt.init_type, \n",
    "                            gpu_ids, n_downsampling=3, opt=opt)\n",
    "\n",
    "    load_network_with_path(netG_A,  'G_A', which_epoch, path)\n",
    "    load_network_with_path(netG_B,  'G_B', which_epoch, path)\n",
    "    load_network_with_path(netG_A_pre,  'G_A_pre', which_epoch, path if opt.pre_path == '' else opt.pre_path)\n",
    "    load_network_with_path(netG_B_pre,  'G_B_pre', which_epoch, path if opt.pre_path == '' else opt.pre_path)\n",
    "        \n",
    "    netG_A.cuda()\n",
    "    netG_B.cuda()\n",
    "    netG_A_pre.cuda()\n",
    "    netG_B_pre.cuda()\n",
    "    \n",
    "    return {'A':netG_A, 'B':netG_B, 'A_pre':netG_A_pre, 'B_pre':netG_B_pre}\n",
    "\n",
    "def get_nets(path, which_epoch = 'latest'):\n",
    "    gpu_ids = [0]\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    opt = load_opt(path)\n",
    "    opt.not_caffe = True\n",
    "    if 'caffe' in path:\n",
    "        opt.not_caffe = False\n",
    "    netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                                  opt.ngf, opt.which_model_netG,\n",
    "                                  opt.norm,  not opt.no_dropout, opt.init_type,\n",
    "                                  gpu_ids, opt=opt)\n",
    "    load_network_with_path(netG_A, 'G_A', which_epoch, path)\n",
    "    \n",
    "    netG_B = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                                  opt.ngf, opt.which_model_netG,\n",
    "                                  opt.norm,  not opt.no_dropout, opt.init_type,\n",
    "                                  gpu_ids, opt=opt)\n",
    "    load_network_with_path(netG_B, 'G_B', which_epoch, path)\n",
    "    \n",
    "    netG_A.cuda()\n",
    "    netG_B.cuda()\n",
    "    \n",
    "    return {'A':netG_A, 'B':netG_B}\n",
    "\n",
    "def get_nets_dis(path, which_epoch = 'latest'):\n",
    "    gpu_ids = [0]\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "    opt = load_opt(path)\n",
    "    opt.caffe=False\n",
    "    netD_A = networks.define_D(opt.output_nc, opt.ndf,\n",
    "                                    opt.which_model_netD,\n",
    "                                    opt.n_layers_D, opt.norm, False, opt.init_type, gpu_ids, opt=opt)\n",
    "    netD_B = networks.define_D(opt.input_nc, opt.ndf,\n",
    "                                    opt.which_model_netD,\n",
    "                                    opt.n_layers_D, opt.norm, False, opt.init_type, gpu_ids, opt=opt)\n",
    "    load_network_with_path(netD_A, 'D_A', which_epoch, path)\n",
    "    load_network_with_path(netD_B, 'D_B', which_epoch, path)\n",
    "    \n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "    \n",
    "    return {'A':netD_A, 'B':netD_B}\n",
    "\n",
    "opt = TestOptions().parse()\n",
    "opt.nThreads = 1   # test code only supports nThreads = 1\n",
    "opt.batchSize = 1  # test code only supports batchSize = 1\n",
    "opt.serial_batches = True  # no shuffle\n",
    "opt.no_flip = True  # no flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.dataroot = './datasets/getchu_aisacelebaselfie_aligned_v6/'\n",
    "phase = 'train'\n",
    "B_path = glob.glob(os.path.join(opt.dataroot,phase+'B/*'))\n",
    "B_mask_path = glob.glob(os.path.join('./datasets/getchu_man_v1_masked_blur/trainB/*'))\n",
    "A_path = glob.glob(os.path.join(opt.dataroot,phase+'A/*'))\n",
    "# AB_path = glob.glob(os.path.join(opt.dataroot,phase+'/*'))\n",
    "\n",
    "img_show, img_pre = get_trans()\n",
    "img_show_64, img_pre_64 = get_trans(64, 64)\n",
    "img_show_128, img_pre_128 = get_trans(128, 128)\n",
    "img_show_256, img_pre_256 = get_trans(256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_1 = get_nets('./tmp/trans/Get_Cel_Ali_Cln_128_d4_0/')\n",
    "# #net_1_d = get_nets_dis('./checkpoints/getchu_celeba_aligned_128/', which_epoch='40')\n",
    "# net_1 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_0/')\n",
    "# net_2 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_1/')\n",
    "net_0 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_0/')\n",
    "net_1 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_0/bk_0122/')\n",
    "net_2 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_0/bk_0123/')\n",
    "net_3 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_1/')\n",
    "net_4 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_0/')\n",
    "net_5 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_1/')\n",
    "net_6 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/')\n",
    "net_7 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/bk_0118/')\n",
    "net_8 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/bk_0122/')\n",
    "net_9 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/bk_0123/')\n",
    "net_10 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/')\n",
    "net_11 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/bk_0118/')\n",
    "net_12 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/bk_0122/')\n",
    "net_13 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_0/')\n",
    "net_14 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_1/')\n",
    "net_15 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_1/bk_0123/')\n",
    "net_16 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_1/bk_0123_v2/')\n",
    "net_17 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_3/')\n",
    "net_18 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_3/bk_0123/')\n",
    "net_19 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_3/')\n",
    "net_20 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_3/bk_0123/')\n",
    "net_21 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_3/bk_0123_v2/')\n",
    "net_22 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_0/')\n",
    "net_23 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_1/')\n",
    "net_24 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_masked_0/')\n",
    "net_25 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_masked_1/')\n",
    "net_26 = get_nets('./checkpoints/load_Get_AisCel_128_4_aff_0/')\n",
    "net_27 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_cont/')\n",
    "net_28 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6/')\n",
    "net_29 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v7/')\n",
    "net_30 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_cont/')\n",
    "net_31 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_v6/')\n",
    "net_32 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_v7/')\n",
    "#net_33 = get_nets('./checkpoints/load_Get_AisCelSelf_128_trued4_aff_gp10_caffe_0/')\n",
    "net_33 = get_nets('/data2/minjunli/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/')\n",
    "net_34 = get_nets('/data2/minjunli/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/')\n",
    "net_35 = get_nets('./checkpoints/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/')\n",
    "net_36 = get_nets('./checkpoints/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/')\n",
    "#net_37 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_color/')\n",
    "net_37 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_color_0/')\n",
    "net_38 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_color_1/')\n",
    "net_39 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_color_2/')\n",
    "net_40 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_idt_0/')\n",
    "net_41 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_idt_1/')\n",
    "net_42 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_idt_color_0/')\n",
    "\n",
    "\n",
    "# net_2 = get_nets('../trans/Get_AisCel_128_d4_gpreweight_1/')\n",
    "# net_1 = get_nets('./checkpoints/load_Get_AisCel_128_4_aff_0/')\n",
    "# #net_1_d = get_nets_dis('./checkpoints/getchu_celeba_aligned_128/', which_epoch='40')\n",
    "# net_2 = get_nets('./checkpoints/load_Get_AisCel_128_4_aff_1/')\n",
    "# net_3 = get_nets('./checkpoints/Get_AisCel_128_4_masked_blur_0/')\n",
    "# net_4 = get_nets('./checkpoints/Get_AisCel_128_4_masked_blur_1/')\n",
    "# net_5 = get_nets('./tmp/trans/Get_AisCel_128_4_0_0//')\n",
    "# net_6 = get_nets('./tmp/trans/Get_AisCel_128_d4_gpreweight_1//')\n",
    "# net_7 = get_nets('./checkpoints/Get_AisCel_128_d4_masked_blur_aff_0/')\n",
    "# net_8 = get_nets('./checkpoints/Get_AisCel_128_d4_masked_blur_aff_1/')\n",
    "# net_9 = get_nets('./checkpoints/Get_AisCel_128_trued4_masked_blur_0/')\n",
    "# net_10 = get_nets('./checkpoints/Get_AisCel_128_trued4_masked_blur_1/')\n",
    "# net_11 = get_nets('./checkpoints/Get_Men_128_aff_0/')\n",
    "# net_12 = get_nets('./checkpoints/Get_Men_128_aff_1/')\n",
    "# net_13 = get_nets('./checkpoints/Get_Men_128_masked_blur_0/')\n",
    "# net_14 = get_nets('./checkpoints/Get_Men_128_masked_blur_1/')\n",
    "\n",
    "# net_11 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_0/')\n",
    "# net_12 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import requests\n",
    "import cStringIO\n",
    "from io import BytesIO\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "URL = 'http://100.102.36.11:30001'\n",
    "\n",
    "\n",
    "def _to_img(img, image_width, image_height):\n",
    "    i = img\n",
    "    if image_width != None and image_height != None:\n",
    "        i = i.resize((image_width, image_height))\n",
    "    buf = cStringIO.StringIO()\n",
    "    i.save(buf, format=\"JPEG\")\n",
    "    buf = base64.encodestring(buf.getvalue())\n",
    "    buf += \"=\" * (-len(buf) % 4)\n",
    "    q = buf\n",
    "    # print(q)\n",
    "    return q\n",
    "\n",
    "def _get_img(img_base64, image_width, image_height):\n",
    "    '''base64 to numpy'''\n",
    "    #print(img_base64)\n",
    "    i = Image.open(BytesIO(base64.decodestring(img_base64))).convert('RGB')\n",
    "    #print(i.size)\n",
    "    imw, imh = i.size\n",
    "    if image_width != None and image_height != None:\n",
    "        i = i.resize((image_width, image_height))\n",
    "    return i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_p = random.choice(B_path)\n",
    "name = B_p.split('/')[-1]\n",
    "\n",
    "img = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "\n",
    "image_width =  None\n",
    "image_height = None\n",
    "\n",
    "image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "print r.status_code\n",
    "#print r.content\n",
    "js = r.json()\n",
    "#print js\n",
    "mask = _get_img(js['prob'], image_width, image_height)\n",
    "#image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "img_npy = np.asarray(img)\n",
    "mask = mask.point(lambda p: p > 50 and 255) \n",
    "#mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "mask_inv_npy = 1 - mask_npy\n",
    "white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "img_masked = Image.fromarray(img_masked_npy)\n",
    "\n",
    "show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(mask), img_pre_128(img_masked)]), show=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test align\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from IPython.html.widgets import interact\n",
    "from PIL import Image\n",
    "import random\n",
    "from PIL import ImageOps\n",
    "\n",
    "def angle_between_2_points(p1, p2):\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    tan = (y2 - y1) / (x2 - x1)\n",
    "    return np.degrees(np.arctan(tan))\n",
    "\n",
    "def align_eye_pad_ailab(im, anno, lambda1, lambda2, lambda3):\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_type = 'edge'\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    x2 = x1 + 2*w\n",
    "    y2 = y1 + h\n",
    "    return tmp_im.crop((x1,y1,x2,y2)).resize((178,218), resample=Image.BICUBIC)\n",
    "\n",
    "def align_eye_pad_ailab_show(im, anno, lambda1, lambda2, lambda3):\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_type = 'edge'\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    xd = 2*w \n",
    "    yd = h\n",
    "    adj = yd * 0.1\n",
    "    return tmp_im.crop(((x1)-((yd-xd)/2)+adj,y1+adj,(x1)-((yd-xd)/2)+yd-adj,y1+yd-adj)).resize((128,128), resample=Image.BICUBIC)\n",
    "\n",
    "def face_detect(img_path, gpu_id=0):\n",
    "    old_ld_library = os.environ['LD_LIBRARY_PATH']\n",
    "    os.environ['LD_LIBRARY_PATH'] = './util/face/lib:./util/face/public_libs:/usr/local/caffe/lib:'+os.environ['LD_LIBRARY_PATH']\n",
    "    tmp = subprocess.check_output(['./util/face/AILab_FaceDemo', img_path, './util/face/models/', str(gpu_id)])\n",
    "    os.environ['LD_LIBRARY_PATH'] = old_ld_library\n",
    "    return eval('{'+''.join(tmp.split('\\n')[1:16])+'}')\n",
    "\n",
    "def test_lambda_2( lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    img_path = '/data2/minjunli/prj/anime/face_lib_points/test/748.jpg'\n",
    "    print img_path\n",
    "    attr = face_detect(img_path)\n",
    "    print attr\n",
    "    im = Image.open(img_path).convert('RGB')\n",
    "    B_ =align_eye_pad_ailab_show(im,attr, lambda1, lambda2, lambda3)\n",
    "    fake_A_1 = test_img(B_, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "    fake_A_2 = test_img(B_, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "    fake_A_5 = test_img(B_, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "    fake_A_6 = test_img(B_, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "    show(torchvision.utils.make_grid([img_pre_128(im), img_pre_128(B_), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu()]))\n",
    "\n",
    "interact(test_lambda_2,  lambda1=(70.0,100.0,1),  lambda2=(210.0,230.0,1),  lambda3=(100.0,120.0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path ='/data2/minjunli/prj/anime/face_lib_points/test/nan2.jpg'\n",
    "print img_path\n",
    "attr = face_detect(img_path)\n",
    "print attr\n",
    "im = Image.open(img_path).convert('RGB')\n",
    "lambda1 = 77.0\n",
    "lambda2 = 228.0\n",
    "lambda3 = 111.0\n",
    "align_eye_pad_ailab_show(im,attr, lambda1, lambda2, lambda3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "list_ = glob.glob('../dst_men/*')\n",
    "lists = []\n",
    "for idx, i in enumerate(list_):\n",
    "    lists.append((i, idx%7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = lists[0]\n",
    "img_path = i[0]\n",
    "img_name = img_path.split('/')[-1]\n",
    "attr = face_detect(img_path, i[1])\n",
    "im = Image.open(img_path).convert('RGB')\n",
    "lambda1 = 77.0\n",
    "lambda2 = 228.0\n",
    "lambda3 = 111.0\n",
    "align_eye_pad_ailab(im,attr, lambda1, lambda2, lambda3).save('../dst_men_aligned/'+img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def test(i):\n",
    "    img_path = i[0]\n",
    "    img_name = img_path.split('/')[-1]\n",
    "    attr = face_detect(img_path, i[1])\n",
    "    im = Image.open(img_path).convert('RGB')\n",
    "    lambda1 = 77.0\n",
    "    lambda2 = 228.0\n",
    "    lambda3 = 111.0\n",
    "    align_eye_pad_ailab(im,attr, lambda1, lambda2, lambda3).save('../dst_men_aligned/'+img_name)\n",
    "\n",
    "pool=Pool(processes=28) \n",
    "pool.map(test,lists)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B_path = glob.glob('./datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur/trainB/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "    img_npy = np.asarray(img)\n",
    "\n",
    "    mask = mask.point(lambda p: p > 50 and 255) \n",
    "    #mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "    mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "    mask_inv_npy = 1 - mask_npy\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "    img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "    img_masked = Image.fromarray(img_masked_npy)\n",
    "    return img_masked\n",
    "\n",
    "def align(img, attr, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    img_aligned =align_eye_pad_ailab(img,attr, lambda1, lambda2, lambda3)\n",
    "    return img_aligned\n",
    "    \n",
    "\n",
    "def align_show(img, attr, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    img_aligned =align_eye_pad_ailab_show(img,attr, lambda1, lambda2, lambda3)\n",
    "    return img_aligned\n",
    "    \n",
    "img_path = random.choice(B_path)\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_1 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=True, drop_eval=False)\n",
    "fake_A_2 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=True, drop_eval=False)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu()]),show=True)\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_5 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_6 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu()]),show=True)\n",
    "\n",
    "\n",
    "fake_A_1 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=True, drop_eval=True)\n",
    "fake_A_2 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=True, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu()]),show=True)\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_5 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=False, drop_eval=False)\n",
    "fake_A_6 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=False)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu()]),show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data_json = json.load(open('../fake_detect/data.json'))\n",
    "rev_attr = {}\n",
    "for i in data_json:\n",
    "    rev_attr[i['ceph_path'].split('/')[-1]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('../fake_detect/haozhi_val/'+data_json[idx]['ceph_path'].split('/')[-1]).resize((128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = ['../fake_detect/haozhi_val/3412c89849f816f68a2caecfdec85cd2.jpg',\n",
    " '../fake_detect/haozhi_val/9bbff8f026e11b964dd3f363c8167a1a.jpg',\n",
    " '../fake_detect/haozhi_val/c876b69809abb6e3a6b040acd9d85d06.jpg',\n",
    " '../fake_detect/haozhi_val/3818efb9fb4a31e1dfb9de3cc07a65b1.jpg',\n",
    " '../fake_detect/haozhi_val/ccdb709e62d2cdb347a04c368c13cbf6.jpg',\n",
    " '../fake_detect/haozhi_val/dd30dcb520c2a960b5b5818f4f4c688d.jpg',\n",
    " '../fake_detect/haozhi_val/d599ea74388468955e4781a38e6cf340.jpg',\n",
    " '../fake_detect/haozhi_val/59a917cc6b4180bbf7fc6ce52a2ffa61.jpg',\n",
    " '../fake_detect/haozhi_val/8dbd4b21667951908e42c989226d68ec.jpg',\n",
    " '../fake_detect/haozhi_val/a6a4f6b4e8d8fb9f59cb3abf15aef259.jpg',\n",
    " '../fake_detect/haozhi_val/b26f66e83d68d25c63df2665ff0ba833.jpg',\n",
    " '../fake_detect/haozhi_val/06b9a10f14c38183c6d716086d7dde37.jpg',\n",
    " '../fake_detect/haozhi_val/863637bae9ff973129205eea6bdc3a91.jpg',\n",
    " '../fake_detect/haozhi_val/12905b2072e761696be4cbccf0b1c647.jpg',\n",
    " '../fake_detect/haozhi_val/23c163554dee92751d7ea557c677650d.jpg',\n",
    " '../fake_detect/haozhi_val/bf3b1877667037662644d15e7ad168c5.jpg',\n",
    " '../fake_detect/haozhi_val/677a444a66ff02568a451318c03c9280.jpg',\n",
    " '../fake_detect/haozhi_val/8ea7d2925deac43e8394da1d83c299d7.jpg',\n",
    " '../fake_detect/haozhi_val/1f7bcec7ab4c2edaa11fca9de813a00e.jpg',\n",
    " '../fake_detect/haozhi_val/fd50522b693b5796385e76c7f7bfd355.jpg',\n",
    " '../fake_detect/haozhi_val/ef45020bd3c87e08dc958e19b7824a49.jpg',\n",
    " '../fake_detect/haozhi_val/9a844f36507160473f50b5ae0462bba0.jpg',\n",
    " '../fake_detect/haozhi_val/5b577804f97ce742ff74dfb26b164d83.jpg',\n",
    " '../mytestimg/myimg/P81503-3069625407.1456271473.jpg',\n",
    " '../face_lib_points/testimg/nan1_aligned.jpg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, img_path in enumerate(tqdm.tqdm_notebook(test_set)):\n",
    "    attr = face_detect(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    img_show = align_show(img, attr)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "    test_net = range(37)\n",
    "\n",
    "    img_show.save('./user_study/orig_%d.png'%idx)\n",
    "    model_count = 0\n",
    "    dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1\n",
    "\n",
    "    \n",
    "    dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "test_set_ex = test_set\n",
    "for idx in tqdm.tqdm_notebook(range(25, 200)):\n",
    "    img_path = random.choice(B_path)\n",
    "    attr = face_detect(img_path)\n",
    "    while rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1 or ('fm1x' not in attr.keys()) or img_path in test_set_ex:\n",
    "        img_path = random.choice(B_path)\n",
    "        attr = face_detect(img_path)\n",
    "    test_set_ex.append(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    img_show = align_show(img, attr)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "    test_net = range(37)\n",
    "\n",
    "    img_show.save('./user_study/orig_%d.png'%idx)\n",
    "    model_count = 0\n",
    "    dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1\n",
    "\n",
    "    \n",
    "    dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "count_idx = 25\n",
    "for img_path in tqdm.tqdm_notebook(filtered_list[:25]):\n",
    "    attr = face_detect(img_path)\n",
    "    while rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1 or ('fm1x' not in attr.keys()) \\\n",
    "        or os.path.join('../fake_detect/haozhi_val',img_path.split('/')[-1]) in test_set:\n",
    "        continue\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    img_show = align_show(img, attr)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "    test_net = range(37)\n",
    "\n",
    "    img_show.save('./user_study/orig_%d.png'%count_idx)\n",
    "    model_count = 0\n",
    "    dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "            model_count += 1\n",
    "\n",
    "    \n",
    "    dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "            model_count += 1\n",
    "    count_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_idx = 24\n",
    "for img_path in filtered_list:\n",
    "    print img_path\n",
    "    attr = face_detect(img_path)\n",
    "    if not(rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1 or ('fm1x' not in attr.keys()) \\\n",
    "        or os.path.join('../fake_detect/haozhi_val',img_path.split('/')[-1]) in test_set):\n",
    "        count_idx += 1\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_aligned = align(img, attr)\n",
    "        img_show = align_show(img, attr)\n",
    "        img_masked = predict_mask_alt(img_aligned)\n",
    "        test_net = range(37)\n",
    "\n",
    "        img_show.save('./user_study/orig_%d.png'%count_idx)\n",
    "        model_count = 0\n",
    "        dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "        for i in test_net:\n",
    "            exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "        for i in test_net:\n",
    "            if i not in dropout:\n",
    "                show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "                model_count += 1\n",
    "\n",
    "        to_show = [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show)]\n",
    "        for i in range(7):\n",
    "            to_show.append(img_pre_128(blank_img))\n",
    "        for i in test_net:\n",
    "            if i not in dropout:\n",
    "                to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "            else:\n",
    "                to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),) / 3'%i))\n",
    "\n",
    "        _ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)\n",
    "\n",
    "\n",
    "        dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "        for i in test_net:\n",
    "            exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "        for i in test_net:\n",
    "            if i not in dropout:\n",
    "                show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "                model_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_list = sorted(glob.glob('./filtered/*'))\n",
    "import tqdm\n",
    "count_idx = 25\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "for img_path in glob.glob('../fake_detect/haozhi_val/*'):\n",
    "    if rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1:\n",
    "        continue\n",
    "    shutil.copy2(img_path, os.path.join('./to_filter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "# B_path = glob.glob('./datasets/getchu_aisa_aligned_cleaned_v1/testB/*')\n",
    "# my_path = glob.glob('../mytestimg/*/*')\n",
    "img_path = './tmp/IMG_4785.JPG'\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan8.jpg'\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan1.jpg'\n",
    "# nan_path = glob.glob('../face_lib_points/testimg/nan*')\n",
    "# img_path = random.choice(nan_path)\n",
    "# while rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1:\n",
    "#     img_path = random.choice(B_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_masked = predict_mask_alt(img_aligned)\n",
    "test_net = range(37)\n",
    "\n",
    "dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "\n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "blank_img = Image.new('RGB',(128,128),(255,255,255))\n",
    "to_show = [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    else:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),) / 3'%i))\n",
    "    \n",
    "_ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_show_ = [ img_pre_128(img_show), ]\n",
    "for i in [3,10,4]:\n",
    "        to_show_.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.append(img_path)\n",
    "print len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_A_2.data[0].cpu().shape\n",
    "\n",
    "\n",
    "def narrow(t, col=1):\n",
    "    tmp = torch.ones(3, 128, col)\n",
    "    to_show_pre = transforms.Compose([transforms.Scale((128,128), Image.BICUBIC)\n",
    "                                      ,transforms.ToTensor()\n",
    "                                      ,transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "    return to_show_pre(Image.fromarray(util.tensor2im(torch.cat((tmp, t, tmp), 2).view(1,3,128,-1))))\n",
    "\n",
    "show(narrow(fake_A_2.data[0].cpu(),10),show=False)\n",
    "# show(pre(narrow(fake_A_2.data[0].cpu(),10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "# my_path = glob.glob('../mytestimg/*/*')\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan12.jpg'\n",
    "# img_path = random.choice(B_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_masked = predict_mask_alt(img_aligned)\n",
    "\n",
    "dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "to_show = [img_pre_128(img), img_pre_128(img_masked), img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "# tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu(), fake_A_7.data[0].cpu(), fake_A_8.data[0].cpu()]),show=True)\n",
    "# to_show = [img_pre_128(img), img_pre_128(img_aligned)]\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    else:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8) / 3'%i))\n",
    "\n",
    "_ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [14,34]:\n",
    "        to_show_.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(torchvision.utils.make_grid(to_show_,nrow=10),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "# my_path = glob.glob('../mytestimg/*/*')\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan12.jpg'\n",
    "# img_path = random.choice(B_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_masked = predict_mask_alt(img_aligned)\n",
    "\n",
    "dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "to_show = [img_pre_128(img), img_pre_128(img_masked), img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "# tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu(), fake_A_7.data[0].cpu(), fake_A_8.data[0].cpu()]),show=True)\n",
    "# to_show = [img_pre_128(img), img_pre_128(img_aligned)]\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    else:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8) / 3'%i))\n",
    "\n",
    "_ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path = glob.glob('../face_lib_points/testimg/xsichen.jpg')\n",
    "# my_path = glob.glob('../mytestimg/seiyuu/*')\n",
    "img_path = random.choice(B_path)\n",
    "# nan_path = glob.glob('../face_lib_points/testimg/nan*')\n",
    "# img_path = random.choice(nan_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "img = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_aligned = predict_mask_alt(img)\n",
    "test_net = range(37,43)\n",
    "\n",
    "dropout = set([4,8,9,15,16,24,25,26])\n",
    "\n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "blank_img = Image.new('RGB',(128,128),(255,255,255))\n",
    "to_show = [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        to_show.append(eval('fake_A_%d.data[0].cpu()'%i))\n",
    "    else:\n",
    "        to_show.append(eval('fake_A_%d.data[0].cpu() / 3'%i))\n",
    "    \n",
    "_ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "def predict_mask_alt(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "    img_npy = np.asarray(img)\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "    softmask = (1-np.array(mask.filter(ImageFilter.GaussianBlur))/255.0)\n",
    "    masked_img = img * (1-softmask) + white_img_npy * (softmask)\n",
    "\n",
    "    return Image.fromarray(masked_img.astype(np.uint8))\n",
    "\n",
    "def predict_mask(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "    img_npy = np.asarray(img)\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "    hardmask = (1-np.array(mask)/255.0)\n",
    "    masked_img = img * (1-hardmask) + white_img_npy * (hardmask)\n",
    "\n",
    "    return Image.fromarray(masked_img.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "B_path = glob.glob('./datasets/getchu_aisacelebaselfie_aligned_v6/trainB/*')\n",
    "tmp_ = None\n",
    "for i in tqdm.tqdm_notebook(range(1000)):\n",
    "    p = random.random()\n",
    "    img_path = B_path[i]\n",
    "#     if p > 0.5:\n",
    "#         img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = img\n",
    "    #fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    fake_A_8 = test_img(img_aligned, net_6['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "\n",
    "    show(fake_A_8.data[0].cpu(),show=False).save('../fake_detect/gliches/'+img_path.split('/')[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir ../fake_detect/gliches0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "\n",
    "def predict_mask_alt(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    #print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "    img_npy = np.asarray(img)\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "    softmask = (1-np.array(mask.filter(ImageFilter.GaussianBlur))/255.0)\n",
    "    masked_img = img * (1-softmask) + white_img_npy * (softmask)\n",
    "\n",
    "    return Image.fromarray(masked_img.astype(np.uint8))\n",
    "\n",
    "B_path = glob.glob('../fake_detect/aligned/*')\n",
    "tmp_ = None\n",
    "# for i in tqdm.tqdm_notebook(range(len(B_path))):\n",
    "p = random.random()\n",
    "img_path = random.choice(B_path)\n",
    "# if 'json' in img_path:\n",
    "#     continue\n",
    "#     attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "#     if 'fm1x' not in attr.keys():\n",
    "#         continue\n",
    "img_aligned = img\n",
    "if p > 0.5:\n",
    "    img_aligned = predict_mask_alt(img_aligned)\n",
    "#fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_8 = test_img(img_aligned, net_16['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "show(fake_A_8.data[0].cpu(),show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path = glob.glob('../fake_detect/filtered/*')\n",
    "tmp_ = None\n",
    "for i in tqdm.tqdm_notebook(range(len(B_path))):\n",
    "    p = random.random()\n",
    "    img_path = B_path[i]\n",
    "    if 'json' in img_path:\n",
    "        continue\n",
    "    attr = face_detect(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    if p > 0.5:\n",
    "        img_aligned = predict_mask_alt(img_aligned)\n",
    "    #fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    fake_A_8 = test_img(img_aligned, net_16['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    show(fake_A_8.data[0].cpu(),show=False).save('../fake_detect/result_filtered/'+img_path.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path = glob.glob('../fake_detect/random/*')\n",
    "tmp_ = None\n",
    "for i in tqdm.tqdm_notebook(range(len(B_path))):\n",
    "    p = random.random()\n",
    "    img_path = B_path[i]\n",
    "    if 'json' in img_path:\n",
    "        continue\n",
    "    attr = face_detect(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    if p > 0.5:\n",
    "        img_aligned = predict_mask_alt(img_aligned)\n",
    "    #fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    fake_A_8 = test_img(img_aligned, net_16['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    show(fake_A_8.data[0].cpu(),show=False).save('../fake_detect/result_random/'+img_path.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = align(Image.open('../face_lib_points/test/nan2.jpg'), attr = face_detect('../face_lib_points/test/nan2.jpg'))\n",
    "tmp2 = Image.open('../face_lib_points/test/nan2.jpg.output.jpg')\n",
    "\n",
    "fake_A_1 = test_img(tmp1, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_2 = test_img(tmp1, net_2['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_3 = test_img(tmp2, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_4 = test_img(tmp2, net_2['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(tmp1), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), \n",
    "                                        img_pre_128(tmp2), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu()], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = align(Image.open('../face_lib_points/test/nan2.jpg'), attr = face_detect('../face_lib_points/test/nan2.jpg'))\n",
    "tmp2 = Image.open('../face_lib_points/test/nan2.jpg.output.jpg')\n",
    "tmpshow = align_eye_pad_ailab_show(Image.open('../face_lib_points/test/nan2.jpg'), anno=face_detect('../face_lib_points/test/nan2.jpg'), lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0)\n",
    "\n",
    "fake_A_1 = test_img(tmp1, net_11['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_2 = test_img(tmp1, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_3 = test_img(tmp2, net_11['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_4 = test_img(tmp2, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(tmpshow), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), \n",
    "                                        img_pre_128(tmpshow), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu()], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_eye_pad_ailab_show(im, anno, lambda1, lambda2, lambda3):\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_type = 'edge'\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    xd = 2*w\n",
    "    yd = h\n",
    "    print xd, yd\n",
    "    return tmp_im.crop((x1-(yd-xd)/2,y1,x1-(yd-xd)/2+yd,y1+yd)).resize((128,128), resample=Image.BICUBIC)\n",
    "\n",
    "tmp = align_eye_pad_ailab_show(Image.open('../face_lib_points/test/nan2.jpg'), anno=face_detect('../face_lib_points/test/nan2.jpg'), lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('../face_lib_points/test/nan2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m = net_12['B']\n",
    "def set_eval(net, bn=True, drop=True):\n",
    "    if isinstance(net, torch.nn.Dropout):\n",
    "        net.training = not drop\n",
    "    elif  isinstance(net, torch.nn.BatchNorm2d) or  isinstance(net, torch.nn.InstanceNorm2d):\n",
    "        net.training = not bn\n",
    "    else:\n",
    "        net.training = False\n",
    "    for i in net.children():\n",
    "        set_eval(i, bn, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_masked = predict_mask(img_aligned)\n",
    "fake_A_3 = test_img(img_masked, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(img_masked, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(img_masked, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(img_masked, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(img_masked, net_9['B'], img_pre_128, 128, eval_mode=True)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img_aligned), img_pre_128(img_masked), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu(), fake_A_7.data[0].cpu(), fake_A_8.data[0].cpu(), fake_A_9.data[0].cpu()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_p = random.choice(B_path)\n",
    "name = B_p.split('/')[-1]\n",
    "# B_p = os.path.join('../../../clean_dataset/out/', B_p)\n",
    "B_ = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "fake_A_1 = test_img(B_, net_1['B'], img_pre_128, 128,eval_mode=True)\n",
    "fake_A_2 = test_img(B_, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(B_, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(B_, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(B_, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(B_, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_10 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "#show(torchvision.utils.make_grid([img_pre_128(B_),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(),fake_A_5.data[0].cpu(),fake_A_6.data[0].cpu()]),show=False)\n",
    "show(torchvision.utils.make_grid([img_pre_128(B_),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(),fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(),fake_A_9.data[0].cpu()]),show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B_p = random.choice(B_path)\n",
    "#name = B_p.split('/')[-1]\n",
    "# B_p = os.path.join('../../../clean_dataset/out/', B_p)\n",
    "\n",
    "\n",
    "def predict_mask(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "    img_npy = np.asarray(img)\n",
    "\n",
    "    mask = mask.point(lambda p: p > 50 and 255) \n",
    "    #mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "    mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "    mask_inv_npy = 1 - mask_npy\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "    img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "    img_masked = Image.fromarray(img_masked_npy)\n",
    "    return img_masked\n",
    "\n",
    "img = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "img_masked = predict_mask(img)\n",
    "fake_A_1 = test_img(img_masked, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_2 = test_img(img_masked, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(img_masked, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(img_masked, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(img_masked, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(img_masked, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(img_masked, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(img_masked, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(img_masked, net_9['B'], img_pre_128, 128, eval_mode=True)\n",
    "#show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(),fake_A_6.data[0].cpu()]),show=False)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(), fake_A_9.data[0].cpu()]),show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def hisEqulColor(img):\n",
    "    img_np = np.array(img)\n",
    "    img_cv = img_np[:,:,::-1]\n",
    "    ycrcb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2YCR_CB)\n",
    "    channels = cv2.split(ycrcb)\n",
    "    cv2.equalizeHist(channels[0], channels[0])\n",
    "    cv2.merge(channels, ycrcb)\n",
    "    img_cv = cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR)\n",
    "    img_np = img_cv[:,:,::-1]\n",
    "    img = Image.fromarray(img_np)\n",
    "    return img\n",
    "\n",
    "B_ = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "B_ = hisEqulColor(B_)\n",
    "\n",
    "fake_A_1 = test_img(B_, net_1['B'], img_pre_128, 128,eval_mode=True)\n",
    "fake_A_2 = test_img(B_, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(B_, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(B_, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(B_, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(B_, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_10 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "show(torchvision.utils.make_grid([img_pre_128(B_), fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(),fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(),fake_A_9.data[0].cpu()]),show=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def hisEqulColor(img):\n",
    "    img_np = np.array(img)\n",
    "    img_cv = img_np[:,:,::-1]\n",
    "    ycrcb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2YCR_CB)\n",
    "    channels = cv2.split(ycrcb)\n",
    "    cv2.equalizeHist(channels[0], channels[0])\n",
    "    cv2.merge(channels, ycrcb)\n",
    "    img_cv = cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR)\n",
    "    img_np = img_cv[:,:,::-1]\n",
    "    img = Image.fromarray(img_np)\n",
    "    return img\n",
    "\n",
    "B_ = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "img = hisEqulColor(B_)\n",
    "\n",
    "image_width =  None\n",
    "image_height = None\n",
    "\n",
    "image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "print r.status_code\n",
    "#print r.content\n",
    "js = r.json()\n",
    "#print js\n",
    "mask = _get_img(js['prob'], image_width, image_height)\n",
    "#image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "img_npy = np.asarray(img)\n",
    "\n",
    "mask = mask.point(lambda p: p > 50 and 255) \n",
    "#mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "mask_inv_npy = 1 - mask_npy\n",
    "white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "img_masked = Image.fromarray(img_masked_npy)\n",
    "\n",
    "\n",
    "fake_A_1 = test_img(img_masked, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_2 = test_img(img_masked, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(img_masked, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(img_masked, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(img_masked, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(img_masked, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(img_masked, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(img_masked, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(img_masked, net_9['B'], img_pre_128, 128, eval_mode=True)\n",
    "#show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(),fake_A_6.data[0].cpu()]),show=False)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(), fake_A_9.data[0].cpu()]),show=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageOps\n",
    "\n",
    "def resize_plot(x=0.37, d=0.87, rot=1.0):\n",
    "    B_ = Image.open(B_p).convert('RGB')#.crop((1445,0,3666,2028))\n",
    "    max_base_width = 512\n",
    "    wpercent = (max_base_width/float(B_.size[0]))\n",
    "    hsize = int((float(B_.size[1])*float(wpercent)))\n",
    "    B_1 = B_.resize((max_base_width,hsize), Image.BICUBIC).rotate(0)\n",
    "    B_2 = B_1\n",
    "    if rot!= 0:\n",
    "        B_2_orig = B_2\n",
    "        np_im = np.array(B_2)\n",
    "        pad_type = 'edge'\n",
    "        pad_size = max(B_2.size[0], B_2.size[1]) / 2\n",
    "        tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "                                  np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                  np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "        B_2 = ImageOps.mirror(tmp_im)\n",
    "        B_2 = B_2.rotate(rot, Image.BICUBIC)\n",
    "        B_2 = B_2.crop((pad_size, pad_size, pad_size+B_2_orig.size[0], pad_size+B_2_orig.size[1]))\n",
    "    B_2 = B_2.resize((int((B_1.size[0])*x),int((B_1.size[1])*(x*d))), Image.BICUBIC)\n",
    "#     B_2_pad = Image.new('RGB',(B_2.size[0]+60, B_2.size[1]+60), (0,0,0))\n",
    "#     np_im = np.array(B_2)\n",
    "#     pad_type = 'mean'\n",
    "#     pad_size = 30\n",
    "#     tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "#                               np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "#                               np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "#     B_2_pad = ImageOps.mirror(tmp_im)\n",
    "#     B_2 = B_2_pad\n",
    "#     print B_2_pad.size, B_2.size\n",
    "    fake_A_1 = test_img(B_2, net_2['B'], img_id_pre, 128, eval_mode=True)\n",
    "    B_2_show = Image.new('RGB', (fake_A_1.shape[-1], fake_A_1.shape[-2]), (0,0,0))\n",
    "    print B_2.size, B_2_show.size\n",
    "    B_2_show.paste(B_2, (0,0))#B_2.getbbox())\n",
    "    show(torchvision.utils.make_grid([img_id_pre(B_2_show),fake_A_1.data[0].cpu()]))\n",
    "\n",
    "interact(resize_plot, x=(0,1,0.01), d=(0.5,1.2,0.01), rot=(-30,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_plot2(x=0.27, d=0.0):\n",
    "    B_ = Image.open(B_p).convert('RGB').crop((129,0,479,464))\n",
    "    base_width = 512\n",
    "    wpercent = (base_width/float(B_.size[0]))\n",
    "    hsize = int((float(B_.size[1])*float(wpercent)))\n",
    "    B_1 = B_.resize((base_width,hsize), Image.BICUBIC)\n",
    "    B_2 = B_1.resize((int((B_1.size[0])*x),int((B_1.size[1])*(x+d))), Image.ANTIALIAS)\n",
    "    fake_A_1 = test_img(B_2, net_2['B'], img_id_pre, 128, eval_mode=True)\n",
    "    B_2_show = Image.new('RGB', (fake_A_1.shape[-1], fake_A_1.shape[-2]), (0,0,0))\n",
    "    print B_2.size, B_2_show.size\n",
    "    B_2_show.paste(B_2, B_2.getbbox())\n",
    "    #print net_1_d['B'].forward(fake_A_1).mean().data \n",
    "    try:\n",
    "        show(torchvision.utils.make_grid([img_id_pre(B_2_show),fake_A_1.data[0].cpu()]))\n",
    "    except Exception:\n",
    "        show(torchvision.utils.make_grid([img_id_pre(B_2_show)]))\n",
    "        show(torchvision.utils.make_grid([fake_A_1.data[0].cpu()]))\n",
    "\n",
    "interact(resize_plot2, x=(0,1,0.01), d=(-0.5,0.5,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1 = get_nets('./checkpoints/Get_AisCel_128_4_aff_1_resize_caffe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = '/data2/minjunli/prj/anime/face_lib_points/test/testimg/test.png'\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_1 = test_img(img_aligned, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu()]),show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb\n",
    "import pdb\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import pytorch2caffe\n",
    "\n",
    "m = net_1['B'].cpu()\n",
    "m.eval()\n",
    "print(m)\n",
    "\n",
    "input_var = Variable(torch.rand(1, 3, 128, 128))\n",
    "output_var = m(input_var)\n",
    "\n",
    "output_dir = 'caffe'\n",
    "pytorch2caffe.pytorch2caffe(input_var, output_var, \n",
    "              os.path.join(output_dir, 'trans.prototxt'),\n",
    "              os.path.join(output_dir, 'trans.caffemodel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pytorch2caffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data2/minjunli/tol/caffe/python/\")\n",
    "sys.path.append(\"/data2/minjunli/tol/pytorch2caffe/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb\n",
    "import pdb\n",
    "pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = caffe.Net('./caffe/trans.prototxt', caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "7e2eb5fb0ab742be98b936bdeadf584c": {
     "views": [
      {
       "cell_index": 79
      }
     ]
    },
    "adc70966e3464c72b2e0d1d1291b64b2": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "cb4d1b442e6942498c4cec7507d090d3": {
     "views": [
      {
       "cell_index": 68
      }
     ]
    },
    "d039635de9544c9e8343be22ad2d2076": {
     "views": [
      {
       "cell_index": 83
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
