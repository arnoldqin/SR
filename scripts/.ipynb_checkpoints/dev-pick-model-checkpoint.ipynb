{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils.util as util\n",
    "import utils.align as align\n",
    "from models import networks\n",
    "from options.test_options import TestOptions\n",
    "import utils.visualize_test as visualize_test\n",
    "\n",
    "opt = TestOptions().parse()\n",
    "dataroot = './datasets/getchu_men_v2/'\n",
    "phase = 'train'\n",
    "B_path = glob.glob(os.path.join(dataroot,phase+'B/*'))\n",
    "B_mask_path = glob.glob(os.path.join('./datasets/getchu_man_v1_masked_blur/trainB/*'))\n",
    "A_path = glob.glob(os.path.join(dataroot,phase+'A/*'))\n",
    "# AB_path = glob.glob(os.path.join(opt.dataroot,phase+'/*'))\n",
    "\n",
    "img_show_128, img_pre_128 = visualize_test.get_trans(128, 128)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_model_list = glob.glob('/data2/minjunli/latest_model/inbox/*')\n",
    "female_model_list +=  glob.glob('/data2/minjunli/latest_model/outbox/Get*')\n",
    "female_model_list += glob.glob('/data2/minjunli/latest_model/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/bk_*')\n",
    "female_model_list += glob.glob('/data2/minjunli/latest_model/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_0/bk_*')\n",
    "female_model_list += glob.glob('/data2/minjunli/latest_model/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/bk_*')\n",
    "female_model_list += glob.glob('/data2/minjunli/latest_model/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_1/bk_*')\n",
    "female_model_list += glob.glob('/data2/minjunli/latest_model/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_3/bk_*')\n",
    "female_model_list += glob.glob('/data2/minjunli/latest_model/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_3/bk_*')\n",
    "female_model_list += glob.glob('/data2/minjunli/latest_model/outbox/load*')\n",
    "female_model_list += glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/Get_Ais*')\n",
    "female_model_list += glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/Get_Ce*')\n",
    "female_model_list += glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/load*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(female_model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_model_list = glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/Get_Men_*')\n",
    "male_model_list += glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/GetE*')\n",
    "male_model_list += glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/NewMale*')\n",
    "male_model_list += glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/lo*load*')\n",
    "male_model_list += glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/loa*Male*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "ok_idx = []\n",
    "failed_idx = []\n",
    "for idx, path in enumerate(tqdm.tqdm_notebook(female_model_list)):\n",
    "    if os.path.isfile(os.path.join(path, 'latest_net_G_A.pth')):\n",
    "        exec(\"net_%d = visualize_test.get_nets(female_model_list[%d], def_opt=opt)\"%(idx,idx))\n",
    "        ok_idx.append(idx)\n",
    "    else:\n",
    "        failed_idx.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "ok_idx_male = []\n",
    "failed_idx_male = []\n",
    "for idx, path in enumerate(tqdm.tqdm_notebook(male_model_list)):\n",
    "    if os.path.isfile(os.path.join(path, 'latest_net_G_A.pth')):\n",
    "        exec(\"net_%d_male = visualize_test.get_nets(male_model_list[%d], def_opt=opt)\"%(idx,idx))\n",
    "        ok_idx_male.append(idx)\n",
    "    else:\n",
    "        failed_idx_male.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.align as align\n",
    "\n",
    "reload(align)\n",
    "\n",
    "def test_lambda_2(net, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    img_path = '/data2/minjunli/tmp/IMG_5321.jpg'\n",
    "    print(img_path)\n",
    "    attr = align.face_detect(img_path)\n",
    "    print(attr)\n",
    "    im = Image.open(img_path).convert('RGB')\n",
    "    B_ = align.align_eye_pad_ailab(im, attr, lambda1, lambda2, lambda3)\n",
    "    fake_A_1 = visualize_test.test_img(B_, net['B'], img_pre_128, 128, eval_mode=True)\n",
    "    visualize_test.show(torchvision.utils.make_grid([img_pre_128(im), img_pre_128(B_), fake_A_1.data[0].cpu()]))\n",
    "\n",
    "test_lambda_2(net_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/data2/minjunli/prj/anime/face_lib_points/test/748.jpg'\n",
    "print img_path\n",
    "attr = align.face_detect(img_path)\n",
    "print attr\n",
    "im = Image.open(img_path).convert('RGB')\n",
    "lambda1 = 77.0\n",
    "lambda2 = 228.0\n",
    "lambda3 = 111.0\n",
    "align.predict_mask(align.align_eye_pad_ailab(im,attr, lambda1, lambda2, lambda3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = '/data2/minjunli/prj/anime/face_lib_points/test/test6.png'\n",
    "\n",
    "\n",
    "\n",
    "align.align(Image.open(impath), align.face_detect(impath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open( '/data2/minjunli/prj/anime/face_lib_points/test/test6.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B_path = glob.glob('./datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur/trainB/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_model_list_rev = {idx:i for idx,i in enumerate(female_model_list)}\n",
    "male_model_list_rev = {idx:i for idx,i in enumerate(male_model_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ok_idx = []\n",
    "for idx, i in enumerate(pass_idx):\n",
    "    if idx in skip_list:\n",
    "        failed_idx.append(i)\n",
    "    else:\n",
    "        ok_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ok_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "failed_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_list =  []\n",
    "for idx, i in enumerate(tqdm.tqdm_notebook(failed_idx)):\n",
    "    try:\n",
    "        exec(\"fake_A_%d = visualize_test.test_img(img_masked, net_%d['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\"%(i,i))\n",
    "        img_list.append(eval(\"fake_A_%d.data[0].cpu()\"%(i)))\n",
    "        print(female_model_list_rev[i], 'reuslt_error')\n",
    "    except:\n",
    "        print(female_model_list_rev[i], 'net_not_found')\n",
    "\n",
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned)] + [img_pre_128(img_blank)] * 8\n",
    "                                                      + img_list, nrow=10),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = [0]\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "opt = util.load_opt('../../cyclegan/pytorch-CycleGAN-and-pix2pix/checkpoints/getchu_celeba_bs1/', opt)\n",
    "opt.not_caffe = True\n",
    "netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                              64, opt.which_model_netG,\n",
    "                              opt.norm,  not True, opt.init_type,\n",
    "                              gpu_ids, opt=opt)\n",
    "util.load_network_with_path(netG_A, 'G_A', path)\n",
    "\n",
    "netG_B = networks.define_G(opt.input_nc, opt.output_nc,\n",
    "                              64, opt.which_model_netG,\n",
    "                              opt.norm,  not True, opt.init_type,\n",
    "                              gpu_ids, opt=opt)\n",
    "util.load_network_with_path(netG_B, 'G_B', path)\n",
    "netG_A.cuda()\n",
    "netG_B.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# B_path = glob.glob('./datasets/etc/celeba/original/img_celeba/*')\n",
    "img_path = random.choice(B_path)\n",
    "img_path = '../datasets/test_img/seiyuu/38757.jpg'\n",
    "# img_path = '../../cyclegan/pytorch-CycleGAN-and-pix2pix/checkpoints/getchu_celeba_bs1/web/images/epoch034_real_B.png'\n",
    "attr = align.face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "img_aligned = align.align(img, attr)\n",
    "img_masked = align.predict_mask_alt(img_aligned)\n",
    "img_blank = Image.fromarray(np.ones((128,128,3)).astype(np.uint8))\n",
    "\n",
    "problems =  {64: 'unable to transfer',\n",
    "             52: 'occasionally unable to transfer',\n",
    "             35: 'mode collapce',\n",
    "             39: 'mode collapce'}\n",
    "good_models = [\n",
    "    (57, '/data2/minjunli/prj/anime/img-trans-pytorch/checkpoints/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1'),\n",
    "    \n",
    "]\n",
    "\n",
    "# mode 1: Not very similar to the input image, and tend to be mode collapce (in most cases output black/brown hair chara)\n",
    "model_mode = {1:[3,4,5,9,12,20,21,25,26,29,41,42,46,51,63]} \n",
    "\n",
    "for idx, i in enumerate(tqdm.tqdm_notebook(ok_idx)):\n",
    "    exec(\"fake_A_%d = visualize_test.test_img(img_aligned, net_%d['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\"%(i,i))\n",
    "\n",
    "img_list = []\n",
    "for idx, i in enumerate(ok_idx):\n",
    "#     if idx in model_mode[1]:\n",
    "#         img_list.append(eval(\"fake_A_%d.data[0].cpu()\"%(i)))\n",
    "    if idx in problems.keys() or idx in model_mode[1]:\n",
    "#         pass\n",
    "        img_list.append(eval(\"fake_A_%d.data[0].cpu() / 5\"%(i)))\n",
    "    else:\n",
    "        img_list.append(eval(\"fake_A_%d.data[0].cpu()\"%(i)))\n",
    "\n",
    "# for idx, i in enumerate(tqdm.tqdm_notebook(ok_idx)):\n",
    "#     exec(\"fake_A_%d = visualize_test.test_img(img_aligned, net_%d['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\"%(i,i))\n",
    "\n",
    "# for idx, i in enumerate(ok_idx):\n",
    "#     if idx in model_mode[1]:\n",
    "#         img_list.append(eval(\"fake_A_%d.data[0].cpu()\"%(i)))\n",
    "\n",
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_masked)] + [img_pre_128(img_blank)] * 7\n",
    "                                                      + img_list, nrow=10),show=True)\n",
    "\n",
    "for i in problems:\n",
    "    print(female_model_list_rev[i], problems[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img_aligned)] + img_list[:10], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img_aligned)] + img_list[10:20], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img_aligned)] + img_list[20:30], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img_aligned)] + img_list[70:], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageOps\n",
    "from IPython.html.widgets import interact\n",
    "reload(align)\n",
    "cidx = ok_idx[49]\n",
    "print(cidx, female_model_list_rev[cidx])\n",
    "def tt(l1 = 77.0, l2 = 228.0, l3 = 111.0):\n",
    "#     img_aligned = img\n",
    "    img_aligned = align.align(img, attr, l1, l2, l3)\n",
    "    img_masked = align.predict_mask_alt(img_aligned)\n",
    "    crop_pos, img_show =  align.align(img, attr, l1, l2, l3, show=True)\n",
    "    for i in [cidx]:\n",
    "        exec(\"fake_A_%d = visualize_test.test_img(img_masked, net_%d['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\"%(i,i))\n",
    "    return visualize_test.show(torchvision.utils.make_grid([img_pre_128(img_show)] + [eval(\"fake_A_%d.data[0].cpu()\"%cidx)] , nrow=6),show=False).resize((1024,512))\n",
    "\n",
    "interact(tt, l1=(70,90,1), l2=(210,240,1), l3=(105,120,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_path = glob.glob('./datasets/etc/haozhi_val/*')\n",
    "# img_path = random.choice(B_path)\n",
    "# img_path = './tmp/FullSizeRender_(1).jpg'\n",
    "reload(align)\n",
    "img_path = '/data2/minjunli/tmp/liyifeng.jpg'\n",
    "attr = align.face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "img_aligned = align.align(img, attr)\n",
    "img_show = align.align(img,attr, show=True)\n",
    "img_masked = align.predict_mask_alt(img_aligned)\n",
    "img_blank = Image.fromarray(np.ones((128,128,3)).astype(np.uint8))\n",
    "\n",
    "problems_male = {}\n",
    "good_models_male = []\n",
    "\n",
    "for idx, i in enumerate(tqdm.tqdm_notebook(ok_idx_male)):\n",
    "    exec(\"fake_A_%d_male = visualize_test.test_img(img_aligned, net_%d_male['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\"%(i,i))\n",
    "\n",
    "img_list_male = []\n",
    "for idx, i in enumerate(ok_idx_male):\n",
    "    tmp = eval(\"fake_A_%d_male.data[0].cpu()\"%(i))\n",
    "    if tmp.shape[-1] != 128:\n",
    "        continue\n",
    "    if idx in problems_male.keys():\n",
    "        img_list_male.append(tmp/5)\n",
    "    else:\n",
    "        img_list_male.append(tmp)\n",
    "\n",
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_show), img_pre_128(img_masked)] \n",
    "                                                      + [img_pre_128(img_blank)] * 0\n",
    "                                                      + img_list_male[-12:], nrow=3),show=True)\n",
    "\n",
    "# for i in problems:\n",
    "#     print(female_model_list_rev[i], problems[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(tqdm.tqdm_notebook(ok_idx_male)):\n",
    "    exec(\"fake_A_%d_male = visualize_test.test_img(img_masked, net_%d_male['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\"%(i,i))\n",
    "\n",
    "img_list_male = []\n",
    "for idx, i in enumerate(ok_idx_male):\n",
    "    tmp = eval(\"fake_A_%d_male.data[0].cpu()\"%(i))\n",
    "    if tmp.shape[-1] != 128:\n",
    "        continue\n",
    "    if idx in problems_male.keys():\n",
    "        img_list_male.append(tmp/5)\n",
    "    else:\n",
    "        img_list_male.append(tmp)\n",
    "\n",
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_show), img_pre_128(img_masked)] \n",
    "                                                      + [img_pre_128(img_blank)] * 0\n",
    "                                                      + [img_list_male[-11],img_list_male[-9],img_list_male[-6]], nrow=3),show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(male_model_list)):\n",
    "    print i, male_model_list_rev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path = glob.glob('./datasets/getchu_men_v5/trainA/*')\n",
    "img_path = random.choice(B_path)\n",
    "# img_path = '../face_lib_points/testimg/nan4.jpg'\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "# img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_1 = test_img(img_aligned, net_1['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_2 = test_img(img_aligned, net_2['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_3 = test_img(img_aligned, net_3['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_4 = test_img(img_aligned, net_4['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_5 = test_img(img_aligned, net_5['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_6 = test_img(img_aligned, net_6['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_7 = test_img(img_aligned, net_7['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_8 = test_img(img_aligned, net_8['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_9 = test_img(img_aligned, net_9['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_10 = test_img(img_aligned, net_10['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_11 = test_img(img_aligned, net_11['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_12 = test_img(img_aligned, net_12['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_13 = test_img(img_aligned, net_13['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_14 = test_img(img_aligned, net_14['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_15 = test_img(img_aligned, net_15['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_16 = test_img(img_aligned, net_16['A'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), \n",
    "                                        fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), \n",
    "                                        fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu(),\n",
    "                                        fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu(),\n",
    "                                        fake_A_7.data[0].cpu(), fake_A_8.data[0].cpu(),\n",
    "                                        fake_A_9.data[0].cpu(), fake_A_10.data[0].cpu(),\n",
    "                                        fake_A_11.data[0].cpu(), fake_A_12.data[0].cpu(),\n",
    "                                        fake_A_13.data[0].cpu(), fake_A_14.data[0].cpu(),\n",
    "                                        fake_A_15.data[0].cpu(), fake_A_16.data[0].cpu()], nrow=6),show=True)\n",
    "\n",
    "# img_aligned = predict_mask_alt(img_aligned)\n",
    "# # img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# # # attr = face_detect(img_path)\n",
    "# # img = Image.open(img_path).convert('RGB')\n",
    "# # img_aligned = img\n",
    "# #img_aligned = align(img, attr)\n",
    "\n",
    "# fake_A_1 = test_img(img_aligned, net_5['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "# fake_A_2 = test_img(img_aligned, net_6['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "# fake_A_3 = test_img(img_aligned, net_7['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "# fake_A_4 = test_img(img_aligned, net_8['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "\n",
    "\n",
    "# tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu()]),show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_example.append(img_path)\n",
    "print bad_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1) \n",
    "\n",
    "class NLayerDiscriminatorMod(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, max_mult=8, norm_layer=nn.BatchNorm2d, feat_len=256):\n",
    "        super(NLayerDiscriminatorMod, self).__init__()\n",
    "        use_bias = False\n",
    "        kw = 3\n",
    "\n",
    "        padw = 1\n",
    "        sequence = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=7, stride=2, padding=3),\n",
    "            norm_layer(ndf),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, max_mult)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult)]\n",
    "            sequence += [nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, max_mult)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
    "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=3, stride=1, padding=padw)]\n",
    "        # sequence += [nn.AvgPool2d(7, stride=1)]\n",
    "        # sequence += [nn.Linear(feat_len, 1)]\n",
    "        sequence += [Flatten(), nn.Linear(feat_len, 1)]\n",
    "\n",
    "        sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "model = NLayerDiscriminatorMod(3, ndf=16, n_layers = 6, max_mult=8, feat_len=4)\n",
    "model = model.cuda()\n",
    "checkpoint = torch.load('../checkpoints/fake_detect_models/dis_mod_v2_all_balanced/chkpt_2600.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                      std=[0.5, 0.5, 0.5])\n",
    "trans = transforms.Compose([\n",
    "    transforms.Scale(128),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B_path = glob.glob('./datasets/etc/manga_portrait_selfie/dst_men/*')\n",
    "img_path = random.choice(B_path)\n",
    "# img_path = './tmp/FullSizeRender_(1).jpg'\n",
    "# img_path = '/data2/minjunli/tmp/clear.jpg'\n",
    "attr = align.face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "img_aligned = align.align(img, attr)\n",
    "img_masked = align.predict_mask_alt(img_aligned)\n",
    "img_blank = Image.fromarray(np.ones((128,128,3)).astype(np.uint8))\n",
    "\n",
    "problems_male = {}\n",
    "model_score = []\n",
    "\n",
    "for idx, i in enumerate(tqdm.tqdm_notebook(ok_idx_male[-12:])):\n",
    "    exec(\"fake_A_%d_male = visualize_test.test_img(img_masked, net_%d_male['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\"%(i,i))\n",
    "    exec(\"tmp_img = trans(Image.fromarray(util.tensor2im(fake_A_%d_male)))\"%i)\n",
    "    input_var = torch.autograd.Variable(tmp_img).cuda()\n",
    "    input_var = input_var.view(1,3,128,128)\n",
    "    with torch.no_grad():\n",
    "        model_score.append(model(input_var))\n",
    "\n",
    "img_list_male = []\n",
    "for idx, i in enumerate(ok_idx_male[-12:]):\n",
    "    tmp = eval(\"fake_A_%d_male.data[0].cpu()\"%(i))\n",
    "    if tmp.shape[-1] != 128:\n",
    "        continue\n",
    "    if idx in problems_male.keys() or model_score[idx] < 0.04:\n",
    "        img_list_male.append(tmp/5)\n",
    "    else:\n",
    "        img_list_male.append(tmp)\n",
    "\n",
    "tmp = visualize_test.show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_masked)] \n",
    "                                                      + [img_pre_128(img_blank)] * 1\n",
    "                                                      + img_list_male[-12:], nrow=4),show=True)\n",
    "for i in range(len(model_score)):\n",
    "    print model_score[i].data[0].cpu(),\n",
    "    if (i+1) % 4 == 0:\n",
    "        print ''\n",
    "# for i in problems:\n",
    "#     print(female_model_list_rev[i], problems[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(util.tensor2im(fake_A_39_male))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "7e2eb5fb0ab742be98b936bdeadf584c": {
     "views": [
      {
       "cell_index": 79
      }
     ]
    },
    "adc70966e3464c72b2e0d1d1291b64b2": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "cb4d1b442e6942498c4cec7507d090d3": {
     "views": [
      {
       "cell_index": 68
      }
     ]
    },
    "d039635de9544c9e8343be22ad2d2076": {
     "views": [
      {
       "cell_index": 83
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
