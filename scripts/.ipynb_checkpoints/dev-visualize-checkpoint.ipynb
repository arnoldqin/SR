{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_0['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.dataroot = './datasets/getchu_aisacelebaselfie_aligned_v6/'\n",
    "phase = 'train'\n",
    "B_path = glob.glob(os.path.join(opt.dataroot,phase+'B/*'))\n",
    "B_mask_path = glob.glob(os.path.join('./datasets/getchu_man_v1_masked_blur/trainB/*'))\n",
    "A_path = glob.glob(os.path.join(opt.dataroot,phase+'A/*'))\n",
    "# AB_path = glob.glob(os.path.join(opt.dataroot,phase+'/*'))\n",
    "\n",
    "img_show, img_pre = get_trans()\n",
    "img_show_64, img_pre_64 = get_trans(64, 64)\n",
    "img_show_128, img_pre_128 = get_trans(128, 128)\n",
    "img_show_256, img_pre_256 = get_trans(256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_1 = get_nets('./tmp/trans/Get_Cel_Ali_Cln_128_d4_0/')\n",
    "# #net_1_d = get_nets_dis('./checkpoints/getchu_celeba_aligned_128/', which_epoch='40')\n",
    "# net_1 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_0/')\n",
    "# net_2 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_1/')\n",
    "net_0 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_0/')\n",
    "net_1 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_0/bk_0122/')\n",
    "net_2 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_0/bk_0123/')\n",
    "net_3 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp10_caffe_1/')\n",
    "net_4 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_0/')\n",
    "net_5 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_1/')\n",
    "net_6 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/')\n",
    "net_7 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/bk_0118/')\n",
    "net_8 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/bk_0122/')\n",
    "net_9 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/bk_0123/')\n",
    "net_10 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/')\n",
    "net_11 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/bk_0118/')\n",
    "net_12 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/bk_0122/')\n",
    "net_13 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_0/')\n",
    "net_14 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_1/')\n",
    "net_15 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_1/bk_0123/')\n",
    "net_16 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_1/bk_0123_v2/')\n",
    "net_17 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_3/')\n",
    "net_18 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_d3_aff_gp2p5_caffe_3/bk_0123/')\n",
    "net_19 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_3/')\n",
    "net_20 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_3/bk_0123/')\n",
    "net_21 = get_nets('/data2/minjunli/outbox/Get_AisCelSelf_128_trued4_aff_gp10_caffe_3/bk_0123_v2/')\n",
    "net_22 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_0/')\n",
    "net_23 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_1/')\n",
    "net_24 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_masked_0/')\n",
    "net_25 = get_nets('./checkpoints/Get_AisCel_128_v6_d4_caffe_aff_masked_1/')\n",
    "net_26 = get_nets('./checkpoints/load_Get_AisCel_128_4_aff_0/')\n",
    "net_27 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_cont/')\n",
    "net_28 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6/')\n",
    "net_29 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v7/')\n",
    "net_30 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_cont/')\n",
    "net_31 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_v6/')\n",
    "net_32 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_v7/')\n",
    "#net_33 = get_nets('./checkpoints/load_Get_AisCelSelf_128_trued4_aff_gp10_caffe_0/')\n",
    "net_33 = get_nets('/data2/minjunli/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/')\n",
    "net_34 = get_nets('/data2/minjunli/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/')\n",
    "net_35 = get_nets('./checkpoints/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0/')\n",
    "net_36 = get_nets('./checkpoints/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1/')\n",
    "#net_37 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_0_v6_color/')\n",
    "net_37 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_masked_idt_0/')\n",
    "net_38 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_masked_idt_1/')\n",
    "net_39 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_masked_idt_2/')\n",
    "net_40 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_masked_idt_3/')\n",
    "net_41 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_masked_ft/')\n",
    "net_42 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_masked_ft_1/')\n",
    "net_43 = get_nets('./checkpoints/load_Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_masked_ft_regp/')\n",
    "net_44 = get_nets('./checkpoints/load_top1_idt_ft/')\n",
    "net_45 = get_nets('./checkpoints/load_top1_lr_ft/')\n",
    "net_46 = get_nets('./checkpoints/load_top2_idt_ft/')\n",
    "net_47 = get_nets('./checkpoints/load_top2_ft_ft/')\n",
    "net_48 = get_nets('./checkpoints/load_top4_idt_ft/')\n",
    "net_49 = get_nets('./checkpoints/load_top4_lr_ft/')\n",
    "net_50 = get_nets('./checkpoints/load_top5_idt_ft/')\n",
    "net_51 = get_nets('./checkpoints/load_top5_lr_ft/')\n",
    "\n",
    "net_52 = get_nets('./checkpoints/Get_AisCelSelf_128_d3_aff_caffe_wgan_dragp/')\n",
    "net_53 = get_nets('./checkpoints/Get_AisCelSelf_128_d3_aff_caffe_wgan_gp/')\n",
    "net_54 = get_nets('./checkpoints/Get_AisCelSelf_128_d3_aff_caffe_wgan_newgp/')\n",
    "net_55 = get_nets('./checkpoints/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_max_ngf_512_0/')\n",
    "net_56 = get_nets('./checkpoints/Get_AisCelSelf_128_d4_aff_gp2p5_caffe_1_max_ngf_512_1/')\n",
    "\n",
    "\n",
    "# net_2 = get_nets('../trans/Get_AisCel_128_d4_gpreweight_1/')\n",
    "# net_1 = get_nets('./checkpoints/load_Get_AisCel_128_4_aff_0/')\n",
    "# #net_1_d = get_nets_dis('./checkpoints/getchu_celeba_aligned_128/', which_epoch='40')\n",
    "# net_2 = get_nets('./checkpoints/load_Get_AisCel_128_4_aff_1/')\n",
    "# net_3 = get_nets('./checkpoints/Get_AisCel_128_4_masked_blur_0/')\n",
    "# net_4 = get_nets('./checkpoints/Get_AisCel_128_4_masked_blur_1/')\n",
    "# net_5 = get_nets('./tmp/trans/Get_AisCel_128_4_0_0//')\n",
    "# net_6 = get_nets('./tmp/trans/Get_AisCel_128_d4_gpreweight_1//')\n",
    "# net_7 = get_nets('./checkpoints/Get_AisCel_128_d4_masked_blur_aff_0/')\n",
    "# net_8 = get_nets('./checkpoints/Get_AisCel_128_d4_masked_blur_aff_1/')\n",
    "# net_9 = get_nets('./checkpoints/Get_AisCel_128_trued4_masked_blur_0/')\n",
    "# net_10 = get_nets('./checkpoints/Get_AisCel_128_trued4_masked_blur_1/')\n",
    "# net_11 = get_nets('./checkpoints/Get_Men_128_aff_0/')\n",
    "# net_12 = get_nets('./checkpoints/Get_Men_128_aff_1/')\n",
    "# net_13 = get_nets('./checkpoints/Get_Men_128_masked_blur_0/')\n",
    "# net_14 = get_nets('./checkpoints/Get_Men_128_masked_blur_1/')\n",
    "\n",
    "# net_11 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_0/')\n",
    "# net_12 = get_nets('./checkpoints/Get_AisCelSel_128_true4_masked_blur_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import requests\n",
    "import cStringIO\n",
    "from io import BytesIO\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "URL = 'http://100.102.36.11:30001'\n",
    "\n",
    "\n",
    "def _to_img(img, image_width, image_height):\n",
    "    i = img\n",
    "    if image_width != None and image_height != None:\n",
    "        i = i.resize((image_width, image_height))\n",
    "    buf = cStringIO.StringIO()\n",
    "    i.save(buf, format=\"JPEG\")\n",
    "    buf = base64.encodestring(buf.getvalue())\n",
    "    buf += \"=\" * (-len(buf) % 4)\n",
    "    q = buf\n",
    "    # print(q)\n",
    "    return q\n",
    "\n",
    "def _get_img(img_base64, image_width, image_height):\n",
    "    '''base64 to numpy'''\n",
    "    #print(img_base64)\n",
    "    i = Image.open(BytesIO(base64.decodestring(img_base64))).convert('RGB')\n",
    "    #print(i.size)\n",
    "    imw, imh = i.size\n",
    "    if image_width != None and image_height != None:\n",
    "        i = i.resize((image_width, image_height))\n",
    "    return i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_p = random.choice(B_path)\n",
    "name = B_p.split('/')[-1]\n",
    "\n",
    "img = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "\n",
    "image_width =  None\n",
    "image_height = None\n",
    "\n",
    "image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "print r.status_code\n",
    "#print r.content\n",
    "js = r.json()\n",
    "#print js\n",
    "mask = _get_img(js['prob'], image_width, image_height)\n",
    "#image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "img_npy = np.asarray(img)\n",
    "mask = mask.point(lambda p: p > 50 and 255) \n",
    "#mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "mask_inv_npy = 1 - mask_npy\n",
    "white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "img_masked = Image.fromarray(img_masked_npy)\n",
    "\n",
    "show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(mask), img_pre_128(img_masked)]), show=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test align\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from IPython.html.widgets import interact\n",
    "from PIL import Image\n",
    "import random\n",
    "from PIL import ImageOps\n",
    "\n",
    "def angle_between_2_points(p1, p2):\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    tan = (y2 - y1) / (x2 - x1)\n",
    "    return np.degrees(np.arctan(tan))\n",
    "\n",
    "def align_eye_pad_ailab(im, anno, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0, pad_type='edge'):\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    if pad_type=='constant':\n",
    "        tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type, constant_values=255), \\\n",
    "                                          np.pad(np_im[:,:,1], pad_size, pad_type, constant_values=255), \\\n",
    "                                          np.pad(np_im[:,:,2], pad_size, pad_type, constant_values=255)]).T, 3))\n",
    "    else:\n",
    "        tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type ), \\\n",
    "                                  np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                  np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    x2 = x1 + 2*w\n",
    "    y2 = y1 + h\n",
    "    return tmp_im.crop((x1,y1,x2,y2)).resize((178,218), resample=Image.BICUBIC)\n",
    "\n",
    "def align_eye_pad_ailab_show(im, anno, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0, pad_type='edge', zoom=0.1):\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    if pad_type=='constant':\n",
    "        tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type, constant_values=255), \\\n",
    "                                          np.pad(np_im[:,:,1], pad_size, pad_type, constant_values=255), \\\n",
    "                                          np.pad(np_im[:,:,2], pad_size, pad_type, constant_values=255)]).T, 3))\n",
    "    else:\n",
    "        tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type ), \\\n",
    "                                  np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                  np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    xd = 2*w \n",
    "    yd = h\n",
    "    adj = yd * zoom\n",
    "    return tmp_im.crop(((x1)-((yd-xd)/2)+adj,y1+adj,(x1)-((yd-xd)/2)+yd-adj,y1+yd-adj)).resize((128,128), resample=Image.BICUBIC)\n",
    "\n",
    "def face_detect(img_path, gpu_id=0):\n",
    "    old_ld_library = os.environ['LD_LIBRARY_PATH']\n",
    "    os.environ['LD_LIBRARY_PATH'] = './util/face/lib:./util/face/public_libs:/usr/local/caffe/lib:'+os.environ['LD_LIBRARY_PATH']\n",
    "    tmp = subprocess.check_output(['./util/face/AILab_FaceDemo', img_path, './util/face/models/', str(gpu_id)])\n",
    "    os.environ['LD_LIBRARY_PATH'] = old_ld_library\n",
    "    return eval('{'+''.join(tmp.split('\\n')[1:16])+'}')\n",
    "\n",
    "def test_lambda_2( lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    img_path = '/data2/minjunli/prj/anime/face_lib_points/test/748.jpg'\n",
    "    print img_path\n",
    "    attr = face_detect(img_path)\n",
    "    print attr\n",
    "    im = Image.open(img_path).convert('RGB')\n",
    "    B_ =align_eye_pad_ailab_show(im,attr, lambda1, lambda2, lambda3)\n",
    "    fake_A_1 = test_img(B_, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "    fake_A_2 = test_img(B_, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "    fake_A_5 = test_img(B_, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "    fake_A_6 = test_img(B_, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "    show(torchvision.utils.make_grid([img_pre_128(im), img_pre_128(B_), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu()]))\n",
    "\n",
    "interact(test_lambda_2,  lambda1=(70.0,100.0,1),  lambda2=(210.0,230.0,1),  lambda3=(100.0,120.0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path ='/data2/minjunli/prj/anime/face_lib_points/test/nan2.jpg'\n",
    "print img_path\n",
    "attr = face_detect(img_path)\n",
    "print attr\n",
    "im = Image.open(img_path).convert('RGB')\n",
    "lambda1 = 77.0\n",
    "lambda2 = 228.0\n",
    "lambda3 = 111.0\n",
    "align_eye_pad_ailab_show(im,attr, lambda1, lambda2, lambda3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "list_ = glob.glob('../dst_men/*')\n",
    "lists = []\n",
    "for idx, i in enumerate(list_):\n",
    "    lists.append((i, idx%7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = lists[0]\n",
    "img_path = i[0]\n",
    "img_name = img_path.split('/')[-1]\n",
    "attr = face_detect(img_path, i[1])\n",
    "im = Image.open(img_path).convert('RGB')\n",
    "lambda1 = 77.0\n",
    "lambda2 = 228.0\n",
    "lambda3 = 111.0\n",
    "align_eye_pad_ailab(im,attr, lambda1, lambda2, lambda3).save('../dst_men_aligned/'+img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def test(i):\n",
    "    img_path = i[0]\n",
    "    img_name = img_path.split('/')[-1]\n",
    "    attr = face_detect(img_path, i[1])\n",
    "    im = Image.open(img_path).convert('RGB')\n",
    "    lambda1 = 77.0\n",
    "    lambda2 = 228.0\n",
    "    lambda3 = 111.0\n",
    "    align_eye_pad_ailab(im,attr, lambda1, lambda2, lambda3).save('../dst_men_aligned/'+img_name)\n",
    "\n",
    "pool=Pool(processes=28) \n",
    "pool.map(test,lists)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B_path = glob.glob('./datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur/trainB/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "    img_npy = np.asarray(img)\n",
    "\n",
    "    mask = mask.point(lambda p: p > 50 and 255) \n",
    "    #mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "    mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "    mask_inv_npy = 1 - mask_npy\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "    img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "    img_masked = Image.fromarray(img_masked_npy)\n",
    "    return img_masked\n",
    "\n",
    "def align(img, attr, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    img_aligned =align_eye_pad_ailab(img,attr, lambda1, lambda2, lambda3)\n",
    "    return img_aligned\n",
    "    \n",
    "\n",
    "def align_show(img, attr, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    img_aligned =align_eye_pad_ailab_show(img,attr, lambda1, lambda2, lambda3)\n",
    "    return img_aligned\n",
    "    \n",
    "img_path = random.choice(B_path)\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_1 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=True, drop_eval=False)\n",
    "fake_A_2 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=True, drop_eval=False)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu()]),show=True)\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_5 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_6 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu()]),show=True)\n",
    "\n",
    "\n",
    "fake_A_1 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=True, drop_eval=True)\n",
    "fake_A_2 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=True, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu()]),show=True)\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = img\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_5 = test_img(img_aligned, net_1['B'], img_pre_128, 128, bn_eval=False, drop_eval=False)\n",
    "fake_A_6 = test_img(img_aligned, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=False)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu()]),show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data_json = json.load(open('../fake_detect/data.json'))\n",
    "rev_attr = {}\n",
    "for i in data_json:\n",
    "    rev_attr[i['ceph_path'].split('/')[-1]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('../fake_detect/haozhi_val/'+data_json[idx]['ceph_path'].split('/')[-1]).resize((128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = ['../fake_detect/haozhi_val/3412c89849f816f68a2caecfdec85cd2.jpg',\n",
    " '../fake_detect/haozhi_val/9bbff8f026e11b964dd3f363c8167a1a.jpg',\n",
    " '../fake_detect/haozhi_val/c876b69809abb6e3a6b040acd9d85d06.jpg',\n",
    " '../fake_detect/haozhi_val/3818efb9fb4a31e1dfb9de3cc07a65b1.jpg',\n",
    " '../fake_detect/haozhi_val/ccdb709e62d2cdb347a04c368c13cbf6.jpg',\n",
    " '../fake_detect/haozhi_val/dd30dcb520c2a960b5b5818f4f4c688d.jpg',\n",
    " '../fake_detect/haozhi_val/d599ea74388468955e4781a38e6cf340.jpg',\n",
    " '../fake_detect/haozhi_val/59a917cc6b4180bbf7fc6ce52a2ffa61.jpg',\n",
    " '../fake_detect/haozhi_val/8dbd4b21667951908e42c989226d68ec.jpg',\n",
    " '../fake_detect/haozhi_val/a6a4f6b4e8d8fb9f59cb3abf15aef259.jpg',\n",
    " '../fake_detect/haozhi_val/b26f66e83d68d25c63df2665ff0ba833.jpg',\n",
    " '../fake_detect/haozhi_val/06b9a10f14c38183c6d716086d7dde37.jpg',\n",
    " '../fake_detect/haozhi_val/863637bae9ff973129205eea6bdc3a91.jpg',\n",
    " '../fake_detect/haozhi_val/12905b2072e761696be4cbccf0b1c647.jpg',\n",
    " '../fake_detect/haozhi_val/23c163554dee92751d7ea557c677650d.jpg',\n",
    " '../fake_detect/haozhi_val/bf3b1877667037662644d15e7ad168c5.jpg',\n",
    " '../fake_detect/haozhi_val/677a444a66ff02568a451318c03c9280.jpg',\n",
    " '../fake_detect/haozhi_val/8ea7d2925deac43e8394da1d83c299d7.jpg',\n",
    " '../fake_detect/haozhi_val/1f7bcec7ab4c2edaa11fca9de813a00e.jpg',\n",
    " '../fake_detect/haozhi_val/fd50522b693b5796385e76c7f7bfd355.jpg',\n",
    " '../fake_detect/haozhi_val/ef45020bd3c87e08dc958e19b7824a49.jpg',\n",
    " '../fake_detect/haozhi_val/9a844f36507160473f50b5ae0462bba0.jpg',\n",
    " '../fake_detect/haozhi_val/5b577804f97ce742ff74dfb26b164d83.jpg',\n",
    " '../mytestimg/myimg/P81503-3069625407.1456271473.jpg',\n",
    " '../face_lib_points/testimg/nan1_aligned.jpg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, img_path in enumerate(tqdm.tqdm_notebook(test_set)):\n",
    "    attr = face_detect(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    img_show = align_show(img, attr)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "    test_net = range(37)\n",
    "\n",
    "    img_show.save('./user_study/orig_%d.png'%idx)\n",
    "    model_count = 0\n",
    "    dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1\n",
    "\n",
    "    \n",
    "    dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "test_set_ex = test_set\n",
    "for idx in tqdm.tqdm_notebook(range(25, 200)):\n",
    "    img_path = random.choice(B_path)\n",
    "    attr = face_detect(img_path)\n",
    "    while rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1 or ('fm1x' not in attr.keys()) or img_path in test_set_ex:\n",
    "        img_path = random.choice(B_path)\n",
    "        attr = face_detect(img_path)\n",
    "    test_set_ex.append(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    img_show = align_show(img, attr)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "    test_net = range(37)\n",
    "\n",
    "    img_show.save('./user_study/orig_%d.png'%idx)\n",
    "    model_count = 0\n",
    "    dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1\n",
    "\n",
    "    \n",
    "    dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(idx,model_count))\n",
    "            model_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "count_idx = 25\n",
    "for img_path in tqdm.tqdm_notebook(filtered_list[:25]):\n",
    "    attr = face_detect(img_path)\n",
    "    while rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1 or ('fm1x' not in attr.keys()) \\\n",
    "        or os.path.join('../fake_detect/haozhi_val',img_path.split('/')[-1]) in test_set:\n",
    "        continue\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    img_show = align_show(img, attr)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "    test_net = range(37)\n",
    "\n",
    "    img_show.save('./user_study/orig_%d.png'%count_idx)\n",
    "    model_count = 0\n",
    "    dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "            model_count += 1\n",
    "\n",
    "    \n",
    "    dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "    for i in test_net:\n",
    "        exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "    for i in test_net:\n",
    "        if i not in dropout:\n",
    "            show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "            model_count += 1\n",
    "    count_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_idx = 24\n",
    "for img_path in filtered_list:\n",
    "    print img_path\n",
    "    attr = face_detect(img_path)\n",
    "    if not(rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1 or ('fm1x' not in attr.keys()) \\\n",
    "        or os.path.join('../fake_detect/haozhi_val',img_path.split('/')[-1]) in test_set):\n",
    "        count_idx += 1\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_aligned = align(img, attr)\n",
    "        img_show = align_show(img, attr)\n",
    "        img_masked = predict_mask_alt(img_aligned)\n",
    "        test_net = range(37)\n",
    "\n",
    "        img_show.save('./user_study/orig_%d.png'%count_idx)\n",
    "        model_count = 0\n",
    "        dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "        for i in test_net:\n",
    "            exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "        for i in test_net:\n",
    "            if i not in dropout:\n",
    "                show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "                model_count += 1\n",
    "\n",
    "        to_show = [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show)]\n",
    "        for i in range(7):\n",
    "            to_show.append(img_pre_128(blank_img))\n",
    "        for i in test_net:\n",
    "            if i not in dropout:\n",
    "                to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "            else:\n",
    "                to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),) / 3'%i))\n",
    "\n",
    "        _ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)\n",
    "\n",
    "\n",
    "        dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "        for i in test_net:\n",
    "            exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "        for i in test_net:\n",
    "            if i not in dropout:\n",
    "                show(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i),show=False).save('./user_study/trans_%d_%d.png'%(count_idx,model_count))\n",
    "                model_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remain_model = []\n",
    "\n",
    "test_net = range(37)\n",
    "\n",
    "model_count = 0\n",
    "dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        remain_model.append((0, i))\n",
    "        model_count += 1\n",
    "\n",
    "dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        remain_model.append((1, i))\n",
    "        model_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[remain_model[i] for i in sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_A_0.data[0].cpu()[img_pre_128(img_aligned)==1.0]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "attrs = json.load(open('./tmp/data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remain_model)\n",
    "sorted_idx =[ 5,  4, 15,  8, 22, 23, 13, 29,  3, 14, 35, 10, 11, 28, 12, 19, 31,\n",
    "              6, 16,  2,  7,  0, 24, 18, 37, 25, 34, 27, 39, 20, 30, 33,  9,  1,\n",
    "             36, 21, 26, 17, 32, 38]\n",
    "keylist = sorted(attrs.keys())\n",
    "# B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "start = 5955\n",
    "for curr_idx, img_path in enumerate(tqdm.tqdm_notebook(keylist[start:])):\n",
    "    attr = attrs[img_path]\n",
    "    if 'y1' not in attr.keys():\n",
    "        continue\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align_eye_pad_ailab(img, attr, pad_type='edge')\n",
    "    crop_pos, orig_pos, img_show = align_eye_pad_ailab_show_alt(img, attr, zoom=0.1, show=False)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "\n",
    "    for idx, i in enumerate(sorted_idx[-4:]):\n",
    "        if remain_model[i][0] == 0:\n",
    "            exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(idx, remain_model[i][1]))\n",
    "        else:\n",
    "            exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(idx, remain_model[i][1]))\n",
    "\n",
    "    for i in range(4):\n",
    "        show(eval(\"post_process(fake_A_%d.data[0].cpu(), orig_pos=orig_pos)\"%i),show=False).save('./tolabel_v2/%d_%d.png'%(i,curr_idx + start))\n",
    "        show(eval(\"post_process(fake_A_%d.data[0].cpu())\"%i),show=False).save('./tolabel_v2_nocrop/%d_%d.png'%(i,curr_idx + start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remain_model)\n",
    "sorted_idx =[ 5,  4, 15,  8, 22, 23, 13, 29,  3, 14, 35, 10, 11, 28, 12, 19, 31,\n",
    "              6, 16,  2,  7,  0, 24, 18, 37, 25, 34, 27, 39, 20, 30, 33,  9,  1,\n",
    "             36, 21, 26, 17, 32, 38]\n",
    "\n",
    "# B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "celeba_path = glob.glob('/data2/minjunli/prj/anime/img-trans-pytorch/datasets/getchu_celeba_aligned_cleaned_v1/trainB/*')\n",
    "# img_path = './FullSizeRender_(1).jpg'\n",
    "img_path = random.choice(celeba_path)\n",
    "# while rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1:\n",
    "#     img_path = random.choice(B_path)\n",
    "# attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "# img_aligned = align_eye_pad_ailab(img, attr, pad_type='edge')\n",
    "img_aligned = img\n",
    "img_show = img\n",
    "# crop_pos, img_show = align_eye_pad_ailab_show_alt(img, attr, zoom=0.1, show=True)\n",
    "#img_aligned = img_show\n",
    "img_masked = img_aligned\n",
    "\n",
    "for idx, i in enumerate(sorted_idx):\n",
    "    if remain_model[i][0] == 0:\n",
    "        exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(idx, remain_model[i][1]))\n",
    "    else:\n",
    "        exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(idx, remain_model[i][1]))\n",
    "\n",
    "blank_img = Image.new('RGB',(128,128),(255,255,255))\n",
    "\n",
    "#to_show = [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show), img_pre_128(img_masked)]\n",
    "# for i in range(4):\n",
    "#     to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "to_show = [img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "for i in range(len(sorted_idx)):\n",
    "    to_show.append(eval('post_process(fake_A_%d.data[0].cpu(),crop_pos=crop_pos)'%i))\n",
    "    #to_show.append(img_pre_128(blank_img))\n",
    "#     to_show.append(eval('fake_A_%d.data[0].cpu()'%i))\n",
    "\n",
    "for i in reversed(range(44,57)):\n",
    "    exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "    to_show.append(eval('post_process(fake_A_%d.data[0].cpu(),crop_pos=crop_pos)'%i))\n",
    "#     exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "#     to_show.append(eval('post_process(fake_A_%d.data[0].cpu(),crop_pos=crop_pos)'%i))\n",
    "\n",
    "# for i in reversed(range(55,57)):\n",
    "#     exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "#     to_show.append(eval('post_process(fake_A_%d.data[0].cpu(),crop_pos=crop_pos)'%i))\n",
    "\n",
    "# img_aligned = align_eye_pad_ailab(img, attr, pad_type='constant')\n",
    "# img_show = align_eye_pad_ailab_show(img, attr, pad_type='constant')\n",
    "# #img_aligned = img_show\n",
    "# img_masked = predict_mask_alt(align_eye_pad_ailab(img, attr))\n",
    "\n",
    "# to_show += [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show), img_pre_128(img_masked)]\n",
    "\n",
    "# for idx, i in enumerate(sorted_idx):\n",
    "#     if remain_model[i][0] == 0:\n",
    "#         exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(idx, remain_model[i][1]))\n",
    "#     else:\n",
    "#         exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(idx, remain_model[i][1]))\n",
    "\n",
    "# for i in range(36,40):\n",
    "#     to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "# #     to_show.append(eval('fake_A_%d.data[0].cpu()'%i))\n",
    "\n",
    "# for j in range(0,20,5):\n",
    "#     to_show += [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show), img_pre_128(img_masked)]\n",
    "#     for i in range(36,40):\n",
    "#         to_show.append(eval('post_process(fake_A_%d.data[0].cpu(),img_aligned,row_del=%d)'%(i,j)))\n",
    "print img_path\n",
    "show(torchvision.utils.make_grid(to_show,nrow=8),show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat = [Image.open(i).resize((128,128), Image.ANTIALIAS) for i in glob.glob('../../ai-yui/teemo_hat/hat*.png')]\n",
    "def alpha_paste_final(ima,imb, x=0,y=0,w=1.0,h=1.0,rot=0):\n",
    "    wsize = int(w * imb.size[0])\n",
    "    hsize = int(h * imb.size[1])\n",
    "    ima = ima.rotate(rot, Image.BICUBIC)\n",
    "    ima = ima.resize((wsize,hsize), Image.ANTIALIAS)\n",
    "    r,g,b,a = ima.split()\n",
    "    im = imb\n",
    "    imb.paste(ima,(x,y),mask = a)\n",
    "    return imb\n",
    "\n",
    "im_l = Image.fromarray(util.tensor2im([to_show[1]]))\n",
    "tmp_show = [img_pre_128(img_show), img_pre_128(alpha_paste_final(hat[1].copy(),im_l.copy(),6,-22,0.9,0.8,-4))]\n",
    "show(torchvision.utils.make_grid(tmp_show,nrow=8),show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(post_process(fake_A_23.data[0].cpu(),crop_pos=crop_pos),show=False).save('./report_show_new/0_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_show.save('./report_show_new/0_orig.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir report_show_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image.open(os.path.join('/data2/minjunli/celeba/original/img_celeba/',img_path.split('/')[-1])).save('./report_show/full_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(img, orig_pos=None, crop_pos=None, col_add=0, row_del=0):\n",
    "    t = img\n",
    "    to_show_pre = transforms.Compose([transforms.Scale((128,128), Image.BICUBIC)\n",
    "                                      ,transforms.ToTensor()\n",
    "                                      ,transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "    if orig_pos is not None:\n",
    "        mask = Image.new('RGB', (128,128), (255,255,255))\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.polygon(orig_pos, fill=(0,0,0))\n",
    "        del draw\n",
    "        t[img_pre_128(mask)==1.0]=1.0\n",
    "    if crop_pos is not None:\n",
    "        crop_pos[0][0] = max(crop_pos[0][0], 0)\n",
    "        crop_pos[0][1] = max(crop_pos[0][1], 0)\n",
    "        crop_pos[1][0] = min(crop_pos[1][0], 128)\n",
    "        crop_pos[1][1] = min(crop_pos[1][1], 128)\n",
    "        padding = int((crop_pos[1][0] - crop_pos[0][0]) * 0.04)\n",
    "        x1 = max(int(crop_pos[0][0]) - padding, 0)\n",
    "        x2 = min(int(crop_pos[1][0]) + padding, 128)\n",
    "        y1 = max(int(crop_pos[0][1]) + padding, 0)\n",
    "        y2 = min(int(crop_pos[1][1]) - padding, 128)\n",
    "        t = t[::,y1:y2,x1:x2]\n",
    "    else:\n",
    "        print 'orig'\n",
    "        crop_pos = [[0,0],[128,128]]\n",
    "        padding = int((crop_pos[1][0] - crop_pos[0][0]) * 0.04)\n",
    "        x1 = max(int(crop_pos[0][0]) - padding, 0)\n",
    "        x2 = min(int(crop_pos[1][0]) + padding, 128)\n",
    "        y1 = max(int(crop_pos[0][1]) + padding, 0)\n",
    "        y2 = min(int(crop_pos[1][1]) - padding, 128)\n",
    "        t = t[::,y1:y2,x1:x2]\n",
    "#         padding = int(128 * 0.07)\n",
    "#         x1 = padding\n",
    "#         x2 = 128 - padding\n",
    "#         y1 = 0\n",
    "#         y2 = 128\n",
    "#         print x1, x2, y1, y2\n",
    "#         t = t[::,y1:y2,x1:x2]\n",
    "    if col_add != 0:\n",
    "        tmp = torch.ones(3, 128, col_add)\n",
    "        t = torch.cat((tmp, t, tmp), 2)\n",
    "    if row_del != 0:\n",
    "        t = t[::,row_del:(128-row_del)]\n",
    "    return to_show_pre(Image.fromarray(util.tensor2im(t.unsqueeze(0))).resize((256,256),Image.LANCZOS).resize((128,128),Image.LANCZOS))\n",
    "\n",
    "show(post_process(fake_A_2.data[0].cpu(), crop_pos = crop_pos),show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_A_0.data[0].cpu()[::,10:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 )\n",
    "\n",
    "def rotate_point(origin, point, angle):\n",
    "    \"\"\"\n",
    "    Rotate a point counterclockwise by a given angle around a given origin.\n",
    "\n",
    "    The angle should be given in radians.\n",
    "    \"\"\"\n",
    "    ox, oy = origin\n",
    "    px, py = point\n",
    "    radius = np.radians(angle)\n",
    "    qx = ox + math.cos(radius) * (px - ox) - math.sin(radius) * (py - oy)\n",
    "    qy = oy + math.sin(radius) * (px - ox) + math.cos(radius) * (py - oy)\n",
    "    return qx, qy\n",
    "\n",
    "def draw_point(draw, p, r=20, fill=(255,0,0)):\n",
    "    if p is None:\n",
    "        return\n",
    "    x, y = p\n",
    "    draw.ellipse(((x-r), (y-r), (x+r), (y+r)), fill=fill)\n",
    "\n",
    "def angle_between_2_points(p1, p2):\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    tan = (y2 - y1) / (x2 - x1)\n",
    "    return np.degrees(np.arctan(tan))\n",
    "    \n",
    "def line_intersection(line1, line2):\n",
    "    xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
    "    ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1]) #Typo was here\n",
    "\n",
    "    def det(a, b):\n",
    "        return a[0] * b[1] - a[1] * b[0]\n",
    "\n",
    "    div = det(xdiff, ydiff)\n",
    "    if div == 0:\n",
    "        return None\n",
    "\n",
    "    d = (det(*line1), det(*line2))\n",
    "    x = det(d, xdiff) / div\n",
    "    y = det(d, ydiff) / div\n",
    "    return x, y    \n",
    "\n",
    "def point_in_rect(p, p1, p2):\n",
    "    return p[0] <= p2[0] and p[0] >= p1[0] and  p[1] <= p2[1] and p[1] >= p1[1]\n",
    "\n",
    "def align_eye_pad_ailab_show_alt(im, anno, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0, zoom=0.1, show=True):\n",
    "    if 'fm1x' not in anno.keys():\n",
    "        im\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_type = 'constant'\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type, constant_values=255), \\\n",
    "                                      np.pad(np_im[:,:,1], pad_size, pad_type, constant_values=255), \\\n",
    "                                      np.pad(np_im[:,:,2], pad_size, pad_type, constant_values=255)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    \n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    xd = 2*w \n",
    "    yd = h\n",
    "    adj = yd * zoom\n",
    "\n",
    "    if show:\n",
    "        crop_x1 = (x1)-((yd-xd)/2)+adj\n",
    "        crop_y1 = y1+adj\n",
    "        crop_x2 = (x1)-((yd-xd)/2)+yd-adj\n",
    "        crop_y2 = y1+yd-adj\n",
    "    else:\n",
    "        crop_x1 = x1\n",
    "        crop_y1 = y1\n",
    "        crop_x2 = x1+xd\n",
    "        crop_y2 = y1+yd\n",
    "    \n",
    "    rotated_anno = {}\n",
    "    for i in range(0,5):\n",
    "        rotated_anno['fm%dx'%i], rotated_anno['fm%dy'%i] = rotate_point((xc, yc), (anno['fm%dx'%i] + pad_size, anno['fm%dy'%i] + pad_size), -angle)\n",
    "\n",
    "    crop_1 = (crop_x1, crop_y1)\n",
    "    crop_2 = (crop_x2, crop_y1)\n",
    "    crop_3 = (crop_x1, crop_y2)\n",
    "    crop_4 = (crop_x2, crop_y2)\n",
    "    \n",
    "    orig_1 = rotate_point((xc, yc), (pad_size, pad_size), -angle)\n",
    "    orig_2 = rotate_point((xc, yc), (pad_size + im.size[0], pad_size), -angle)\n",
    "    orig_3 = rotate_point((xc, yc), (pad_size, pad_size + im.size[1]), -angle)\n",
    "    orig_4 = rotate_point((xc, yc), (pad_size + im.size[0], pad_size + im.size[1]), -angle)\n",
    "    \n",
    "    center = (xc, yc)\n",
    "\n",
    "    intersect = []\n",
    "    dist = 999999.0\n",
    "    nearest_point = None\n",
    "    for i,j in [(1,2), (2,4), (1,3), (3,4)]:\n",
    "        for k in (i,j):\n",
    "            intersect.append((k,eval('line_intersection((orig_%d, orig_%d), (crop_%d, center))'%(i,j,k))))\n",
    "            d = distance(intersect[-1][-1], center)\n",
    "            if d < dist:\n",
    "                dist = d\n",
    "                nearest_point = intersect[-1]\n",
    "    \n",
    "    small_crop = False\n",
    "    for i in intersect:\n",
    "        if point_in_rect(i[-1], (crop_x1, crop_y1), (crop_x2, crop_y2)):\n",
    "            small_crop = True\n",
    "            break\n",
    "\n",
    "    if nearest_point[0] == 1:\n",
    "        x_tmp = crop_2\n",
    "        y_tmp = crop_3\n",
    "    if nearest_point[0] == 2:\n",
    "        x_tmp = crop_1\n",
    "        y_tmp = crop_4\n",
    "    if nearest_point[0] == 3:\n",
    "        x_tmp = crop_4\n",
    "        y_tmp = crop_1\n",
    "    if nearest_point[0] == 4:\n",
    "        x_tmp = crop_3\n",
    "        y_tmp = crop_2\n",
    "    \n",
    "    x2, _ = line_intersection((nearest_point[1], (nearest_point[1][0]+1, nearest_point[1][1])), (x_tmp, center))\n",
    "    _, y2 = line_intersection((nearest_point[1], (nearest_point[1][0], nearest_point[1][1]+1)), (y_tmp, center))\n",
    "\n",
    "    final_crop_x1 = min(nearest_point[1][0], x2)\n",
    "    final_crop_y1 = min(nearest_point[1][1], y2)\n",
    "    final_crop_x2 = max(nearest_point[1][0], x2)\n",
    "    final_crop_y2 = max(nearest_point[1][1], y2)\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        if not point_in_rect((rotated_anno['fm%dx'%i],rotated_anno['fm%dy'%i]), (final_crop_x1, final_crop_y1), (final_crop_x2, final_crop_y2)):\n",
    "            print 'Too Close to Boundary'\n",
    "            tmp_im = Image.fromarray(np.array(tmp_im) / 4)\n",
    "            break\n",
    "    \n",
    "    crop_pos = []\n",
    "    x_ratio = 128.0 / (crop_x2 - crop_x1)\n",
    "    y_ratio = 128.0 / (crop_y2 - crop_y1)\n",
    "    crop_pos.append([(final_crop_x1 - crop_x1) * x_ratio, (final_crop_y1 - crop_y1) * y_ratio])\n",
    "    crop_pos.append([(final_crop_x2 - crop_x1) * x_ratio, (final_crop_y2 - crop_y1) * y_ratio])\n",
    "    \n",
    "    orig_pos = []\n",
    "    orig_pos.append(((orig_1[0] - crop_x1) * x_ratio, (orig_1[1] - crop_y1) * y_ratio))\n",
    "    orig_pos.append(((orig_2[0] - crop_x1) * x_ratio, (orig_2[1] - crop_y1) * y_ratio))\n",
    "    orig_pos.append(((orig_4[0] - crop_x1) * x_ratio, (orig_4[1] - crop_y1) * y_ratio))\n",
    "    orig_pos.append(((orig_3[0] - crop_x1) * x_ratio, (orig_3[1] - crop_y1) * y_ratio))\n",
    "#     print crop_pos\n",
    "#     print orig_pos\n",
    "\n",
    "   # return crop_pos, orig_pos, tmp_im.crop((crop_x1,crop_y1,crop_x2,crop_y2)).resize((128,128), resample=Image.BICUBIC)\n",
    "    \n",
    "    if not small_crop:\n",
    "        return crop_pos, tmp_im.crop((crop_x1,crop_y1,crop_x2,crop_y2)).resize((128,128), resample=Image.BICUBIC)\n",
    "    else:\n",
    "        print 'sm'\n",
    "        return crop_pos, tmp_im.crop((final_crop_x1,final_crop_y1,final_crop_x2,final_crop_y2)).resize((128,128), resample=Image.BICUBIC)\n",
    "\n",
    "to_show = []\n",
    "for i in tqdm.tqdm_notebook(range(8)):\n",
    "    img_path = random.choice(B_path)\n",
    "    attr = face_detect(img_path)\n",
    "    crop_pos, _, aligned_img = align_eye_pad_ailab_show_alt(Image.open(img_path), attr, show=False)\n",
    "    to_show.append(img_pre_128(aligned_img))\n",
    "\n",
    "show(torchvision.utils.make_grid(to_show),show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = Image.open(random.choice(B_path)) \n",
    "im = Image.fromarray(np.array(im) / 4)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import math\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2 )\n",
    "\n",
    "def rotate_point(origin, point, angle):\n",
    "    \"\"\"\n",
    "    Rotate a point counterclockwise by a given angle around a given origin.\n",
    "\n",
    "    The angle should be given in radians.\n",
    "    \"\"\"\n",
    "    ox, oy = origin\n",
    "    px, py = point\n",
    "    radius = np.radians(angle)\n",
    "    qx = ox + math.cos(radius) * (px - ox) - math.sin(radius) * (py - oy)\n",
    "    qy = oy + math.sin(radius) * (px - ox) + math.cos(radius) * (py - oy)\n",
    "    return qx, qy\n",
    "\n",
    "def draw_point(draw, p, r=20, fill=(255,0,0)):\n",
    "    if p is None:\n",
    "        return\n",
    "    x, y = p\n",
    "    draw.ellipse(((x-r), (y-r), (x+r), (y+r)), fill=fill)\n",
    "\n",
    "def angle_between_2_points(p1, p2):\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    tan = (y2 - y1) / (x2 - x1)\n",
    "    return np.degrees(np.arctan(tan))\n",
    "    \n",
    "def line_intersection(line1, line2):\n",
    "    xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
    "    ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1]) #Typo was here\n",
    "\n",
    "    def det(a, b):\n",
    "        return a[0] * b[1] - a[1] * b[0]\n",
    "\n",
    "    div = det(xdiff, ydiff)\n",
    "    if div == 0:\n",
    "        return None\n",
    "\n",
    "    d = (det(*line1), det(*line2))\n",
    "    x = det(d, xdiff) / div\n",
    "    y = det(d, ydiff) / div\n",
    "    return x, y    \n",
    "\n",
    "def point_in_rect(p, p1, p2):\n",
    "    return p[0] <= p2[0] and p[0] >= p1[0] and  p[1] <= p2[1] and p[1] >= p1[1]\n",
    "\n",
    "def align_eye_pad_ailab_show_alt(im, anno, lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0):\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_type = 'constant'\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type, constant_values=255), \\\n",
    "                                      np.pad(np_im[:,:,1], pad_size, pad_type, constant_values=255), \\\n",
    "                                      np.pad(np_im[:,:,2], pad_size, pad_type, constant_values=255)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    \n",
    "    draw = ImageDraw.Draw(tmp_im) ##################\n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    xd = 2*w \n",
    "    yd = h\n",
    "    adj = yd * 0.1\n",
    "#     adj_boundary_error = yd * 0.17\n",
    "\n",
    "    crop_x1 = (x1)-((yd-xd)/2)+adj\n",
    "    crop_y1 = y1+adj\n",
    "    crop_x2 = (x1)-((yd-xd)/2)+yd-adj\n",
    "    crop_y2 = y1+yd-adj\n",
    "    \n",
    "    rotated_anno = {}\n",
    "    for i in range(0,5):\n",
    "        rotated_anno['fm%dx'%i], rotated_anno['fm%dy'%i] = rotate_point((xc, yc), (anno['fm%dx'%i] + pad_size, anno['fm%dy'%i] + pad_size), -angle)\n",
    "#     boundary_x1 = (x1)-((yd-xd)/2)+adj_boundary_error\n",
    "#     boundary_y1 = y1+adj_boundary_error\n",
    "#     boundary_x2 = (x1)-((yd-xd)/2)+yd-adj_boundary_error\n",
    "#     boundary_y2 = y1+yd-adj_boundary_error\n",
    "    \n",
    "#     draw_point(draw, (boundary_x1, boundary_y1), fill=(255,0,255))\n",
    "#     draw_point(draw, (boundary_x1, boundary_y2), fill=(255,0,255))\n",
    "#     draw_point(draw, (boundary_x2, boundary_y2), fill=(255,0,255))\n",
    "#     draw_point(draw, (boundary_x2, boundary_y1), fill=(255,0,255))\n",
    "    \n",
    "    crop_1 = (crop_x1, crop_y1)\n",
    "    crop_2 = (crop_x2, crop_y1)\n",
    "    crop_3 = (crop_x1, crop_y2)\n",
    "    crop_4 = (crop_x2, crop_y2)\n",
    "    \n",
    "    draw_point(draw, rotate_point((xc, yc), (pad_size, pad_size), -angle), fill=(0,255,0))\n",
    "    draw_point(draw, rotate_point((xc, yc), (pad_size + im.size[0], pad_size), -angle), fill=(0,255,0))\n",
    "    draw_point(draw, rotate_point((xc, yc), (pad_size, pad_size + im.size[1]), -angle), fill=(0,255,0))\n",
    "    draw_point(draw, rotate_point((xc, yc), (pad_size + im.size[0], pad_size + im.size[1]), -angle), fill=(0,255,0))\n",
    "    \n",
    "    orig_1 = rotate_point((xc, yc), (pad_size, pad_size), -angle)\n",
    "    orig_2 = rotate_point((xc, yc), (pad_size + im.size[0], pad_size), -angle)\n",
    "    orig_3 = rotate_point((xc, yc), (pad_size, pad_size + im.size[1]), -angle)\n",
    "    orig_4 = rotate_point((xc, yc), (pad_size + im.size[0], pad_size + im.size[1]), -angle)\n",
    "    \n",
    "    center = (xc, yc)\n",
    "    draw_point(draw, line_intersection((orig_1, orig_2), (crop_1, center)), fill=(0,0,255))\n",
    "    draw_point(draw, line_intersection((orig_1, orig_2), (crop_2, center)), fill=(0,0,255))\n",
    "    draw_point(draw, line_intersection((orig_2, orig_4), (crop_2, center)), fill=(0,0,255))\n",
    "    draw_point(draw, line_intersection((orig_2, orig_4), (crop_4, center)), fill=(0,0,255))\n",
    "    draw_point(draw, line_intersection((orig_1, orig_3), (crop_3, center)), fill=(0,0,255))\n",
    "    draw_point(draw, line_intersection((orig_1, orig_3), (crop_1, center)), fill=(0,0,255))\n",
    "    draw_point(draw, line_intersection((orig_3, orig_4), (crop_4, center)), fill=(0,0,255))\n",
    "    draw_point(draw, line_intersection((orig_3, orig_4), (crop_3, center)), fill=(0,0,255))\n",
    "    \n",
    "    intersect = []\n",
    "    dist = 999999.0\n",
    "    nearest_point = None\n",
    "    for i,j in [(1,2), (2,4), (1,3), (3,4)]:\n",
    "        for k in (i,j):\n",
    "            intersect.append((k,eval('line_intersection((orig_%d, orig_%d), (crop_%d, center))'%(i,j,k))))\n",
    "            d = distance(intersect[-1][-1], center)\n",
    "            if d < dist:\n",
    "                dist = d\n",
    "                nearest_point = intersect[-1]\n",
    "    \n",
    "    small_crop = False\n",
    "    for i in intersect:\n",
    "        if point_in_rect(i[-1], (crop_x1, crop_y1), (crop_x2, crop_y2)):\n",
    "            print 'Small Crop'\n",
    "            small_crop = True\n",
    "            break\n",
    "    \n",
    "    print nearest_point[0]\n",
    "    if nearest_point[0] == 1:\n",
    "        x_tmp = crop_2\n",
    "        y_tmp = crop_3\n",
    "    if nearest_point[0] == 2:\n",
    "        x_tmp = crop_1\n",
    "        y_tmp = crop_4\n",
    "    if nearest_point[0] == 3:\n",
    "        x_tmp = crop_4\n",
    "        y_tmp = crop_1\n",
    "    if nearest_point[0] == 4:\n",
    "        x_tmp = crop_3\n",
    "        y_tmp = crop_2\n",
    "    \n",
    "    x2, _ = line_intersection((nearest_point[1], (nearest_point[1][0]+1, nearest_point[1][1])), (x_tmp, center))\n",
    "    _, y2 = line_intersection((nearest_point[1], (nearest_point[1][0], nearest_point[1][1]+1)), (y_tmp, center))\n",
    "\n",
    "    draw_point(draw, nearest_point[1], fill=(255,255,0))\n",
    "    draw_point(draw, (nearest_point[1][0], y2), fill=(0,255,255))\n",
    "    draw_point(draw, (x2, y2), fill=(0,255,255))\n",
    "    draw_point(draw, (x2, nearest_point[1][1]), fill=(0,255,255))\n",
    "#     draw_point(draw, (max(orig_1[0], crop_1[0]), max(orig_1[1], crop_1[1])), fill=(0,0,255))\n",
    "#     draw_point(draw, (min(orig_2[0], crop_2[0]), max(orig_1[1], crop_1[1])), fill=(0,0,255))\n",
    "#     draw_point(draw, (max(orig_1[0], crop_1[0]), min(orig_4[1], crop_4[1])), fill=(0,0,255))\n",
    "#     draw_point(draw, (min(orig_2[0], crop_2[0]), min(orig_4[1], crop_4[1])), fill=(0,0,255))\n",
    "    \n",
    "    draw_point(draw, crop_1)\n",
    "    draw_point(draw, crop_2)\n",
    "    draw_point(draw, crop_3)\n",
    "    draw_point(draw, crop_4)\n",
    "    \n",
    "    if not small_crop:\n",
    "        final_crop_x1 = crop_x1\n",
    "        final_crop_y1 = crop_y1\n",
    "        final_crop_x2 = crop_x2\n",
    "        final_crop_y2 = crop_y2\n",
    "    else:\n",
    "        final_crop_x1 = min(nearest_point[1][0], x2)\n",
    "        final_crop_y1 = min(nearest_point[1][1], y2)\n",
    "        final_crop_x2 = max(nearest_point[1][0], x2)\n",
    "        final_crop_y2 = max(nearest_point[1][1], y2)\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        draw_point(draw, (rotated_anno['fm%dx'%i],rotated_anno['fm%dy'%i]), fill=(255,0,255))\n",
    "        if not point_in_rect((rotated_anno['fm%dx'%i],rotated_anno['fm%dy'%i]), (final_crop_x1, final_crop_y1), (final_crop_x2, final_crop_y2)):\n",
    "            print 'Too Close to Boundary'\n",
    "            break\n",
    "    del draw #####################################\n",
    "    \n",
    "    crop_pos = []\n",
    "    x_ratio = 128.0 / (crop_x2 - crop_x1)\n",
    "    y_ratio = 128.0 / (crop_y2 - crop_y1)\n",
    "    crop_pos.append([(final_crop_x1 - crop_x1) * x_ratio, (final_crop_y1 - crop_y1) * y_ratio])\n",
    "    crop_pos.append([(final_crop_x2 - crop_x1) * x_ratio, (final_crop_y2 - crop_y1) * y_ratio])\n",
    "    \n",
    "    orig_pos = []\n",
    "    orig_pos.append(((orig_1[0] - crop_x1) * x_ratio, (orig_1[1] - crop_y1) * y_ratio))\n",
    "    orig_pos.append(((orig_2[0] - crop_x1) * x_ratio, (orig_2[1] - crop_y1) * y_ratio))\n",
    "    orig_pos.append(((orig_4[0] - crop_x1) * x_ratio, (orig_4[1] - crop_y1) * y_ratio))\n",
    "    orig_pos.append(((orig_3[0] - crop_x1) * x_ratio, (orig_3[1] - crop_y1) * y_ratio))\n",
    "    print crop_pos\n",
    "    print orig_pos\n",
    "\n",
    "    return crop_pos, orig_pos, tmp_im.crop((crop_x1,crop_y1,crop_x2,crop_y2)).resize((128,128), resample=Image.BICUBIC)\n",
    "    if not small_crop:\n",
    "        return crop_pos, orig_pos, tmp_im.crop((crop_x1,crop_y1,crop_x2,crop_y2)).resize((128,128), resample=Image.BICUBIC)\n",
    "    else:\n",
    "        return crop_pos, orig_pos, tmp_im.crop((final_crop_x1,final_crop_y1,final_crop_x2,final_crop_y2)).resize((128,128), resample=Image.BICUBIC)\n",
    "\n",
    "img_path = random.choice(B_path)\n",
    "attr = face_detect(img_path)\n",
    "a, b, c = align_eye_pad_ailab_show_alt(Image.open(img_path), attr)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = c\n",
    "mask = Image.new('RGB', img.size, (255,255,255))\n",
    "draw = ImageDraw.Draw(mask)\n",
    "draw.polygon(b, fill=(0,0,0))\n",
    "del draw\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_show_idt, img_pre_idt = get_idtrans()\n",
    "# img_path = random.choice(B_path)\n",
    "# attr = face_detect(img_path)\n",
    "full_img = align_eye_pad_ailab_full(Image.open(img_path), attr)\n",
    "_ = show(torchvision.utils.make_grid([img_pre_idt(full_img)]),show=True)\n",
    "_ = show(torchvision.utils.make_grid([test_img(full_img, net_32['B'], img_pre_idt, 128, bn_eval=False, drop_eval=True).data[0].cpu()]) ,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_list = sorted(glob.glob('./filtered/*'))\n",
    "import tqdm\n",
    "count_idx = 25\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "for img_path in glob.glob('../fake_detect/haozhi_val/*'):\n",
    "    if rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1:\n",
    "        continue\n",
    "    shutil.copy2(img_path, os.path.join('./to_filter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "# B_path = glob.glob('./datasets/getchu_aisa_aligned_cleaned_v1/testB/*')\n",
    "# my_path = glob.glob('../mytestimg/*/*')\n",
    "# img_path = random.choice(B_path)\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan8.jpg'\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan1.jpg'\n",
    "# nan_path = glob.glob('../face_lib_points/testimg/nan*')\n",
    "# img_path = random.choice(nan_path)\n",
    "# while rev_attr[img_path.split('/')[-1]]['machanno:FaceFullAnnotator']['data'][0]['gender'] == 1:\n",
    "#     img_path = random.choice(B_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_masked = predict_mask_alt(img_aligned)\n",
    "test_net = range(37)\n",
    "\n",
    "dropout = set([7,4,8,9,13,15,12,16,24,25,26,35,32,28,29,18,31,36,23])\n",
    "\n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "blank_img = Image.new('RGB',(128,128),(255,255,255))\n",
    "to_show = [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    else:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),) / 3'%i))\n",
    "    \n",
    "show(torchvision.utils.make_grid(to_show,nrow=10),show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.append(img_path)\n",
    "print len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_A_2.data[0].cpu().shape\n",
    "\n",
    "def narrow_cut(t, col=1):\n",
    "    tmp = torch.ones(3, 128, col)\n",
    "    to_show_pre = transforms.Compose([transforms.Scale((128,128), Image.BICUBIC)\n",
    "                                      ,transforms.ToTensor()\n",
    "                                      ,transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "    return to_show_pre(Image.fromarray(util.tensor2im(torch.cat((tmp, t, tmp), 2).view(1,3,128,-1))))\n",
    "\n",
    "def narrow(t, col=1):\n",
    "    tmp = torch.ones(3, 128, col)\n",
    "    to_show_pre = transforms.Compose([transforms.Scale((128,128), Image.BICUBIC)\n",
    "                                      ,transforms.ToTensor()\n",
    "                                      ,transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "    return to_show_pre(Image.fromarray(util.tensor2im(torch.cat((tmp, t, tmp), 2).view(1,3,128,-1))))\n",
    "\n",
    "show(narrow(fake_A_2.data[0].cpu(),10),show=False)\n",
    "# show(pre(narrow(fake_A_2.data[0].cpu(),10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "# my_path = glob.glob('../mytestimg/*/*')\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan12.jpg'\n",
    "img_path = random.choice(B_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_masked = predict_mask_alt(img_aligned)\n",
    "\n",
    "dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "to_show = [img_pre_128(img), img_pre_128(img_masked), img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "for i in range(37,44):\n",
    "    exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "# tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu(), fake_A_7.data[0].cpu(), fake_A_8.data[0].cpu()]),show=True)\n",
    "# to_show = [img_pre_128(img), img_pre_128(img_aligned)]\n",
    "for i in range(37,44):\n",
    "    if i not in dropout:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    else:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8) / 3'%i))\n",
    "\n",
    "_ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "B_path = glob.glob('../fake_detect/haozhi_val/*')\n",
    "# my_path = glob.glob('../mytestimg/*/*')\n",
    "# img_path = '/data2/minjunli/inbox/testimg/nan12.jpg'\n",
    "# img_path = random.choice(B_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_masked = predict_mask_alt(img_aligned)\n",
    "\n",
    "dropout = set([8,9,13,26,16,28,29,5,35,12,19,18,7,25,10])\n",
    "\n",
    "to_show = [img_pre_128(img), img_pre_128(img_masked), img_pre_128(img_show)]\n",
    "for i in range(7):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "# img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "# attr = face_detect(img_path)\n",
    "#img_aligned = align(img, attr)\n",
    "\n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "# tmp = show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(), fake_A_6.data[0].cpu(), fake_A_7.data[0].cpu(), fake_A_8.data[0].cpu()]),show=True)\n",
    "# to_show = [img_pre_128(img), img_pre_128(img_aligned)]\n",
    "for i in test_net:\n",
    "    if i not in dropout:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    else:\n",
    "        to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8) / 3'%i))\n",
    "\n",
    "_ = show(torchvision.utils.make_grid(to_show,nrow=10),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_path = glob.glob('./datasets/getchu_aisa_aligned_cleaned_v1/trainB/*')\n",
    "# my_path = glob.glob('../mytestimg/seiyuu/*')\n",
    "# img_path = random.choice(B_path)\n",
    "# nan_path = glob.glob('../face_lib_points/testimg/nan*')\n",
    "# img_path = random.choice(nan_path)\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "img_aligned = align(img, attr)\n",
    "img_show = align_show(img, attr)\n",
    "# img_aligned = img\n",
    "img_masked = predict_mask_alt(img_aligned)\n",
    "\n",
    "test_net = [33,28,37,38,39,40,41,42,43]\n",
    "\n",
    "dropout = set([4,8,9,15,16,24,25,26])\n",
    "\n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_aligned, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "blank_img = Image.new('RGB',(128,128),(255,255,255))\n",
    "to_show = [img_pre_128(img), img_pre_128(img_aligned), img_pre_128(img_show)]\n",
    "for i in range(6):\n",
    "    to_show.append(img_pre_128(blank_img))\n",
    "\n",
    "for i in test_net:\n",
    "    to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "        \n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_masked, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "for i in test_net:\n",
    "    to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    \n",
    "for i in test_net:\n",
    "    exec('fake_A_%d = test_img(img_show, net_%d[\\'B\\'], img_pre_128, 128, bn_eval=False, drop_eval=True)'%(i,i))\n",
    "\n",
    "for i in test_net:\n",
    "    to_show.append(eval('narrow(fake_A_%d.data[0].cpu(),8)'%i))\n",
    "    \n",
    "    \n",
    "_ = show(torchvision.utils.make_grid(to_show,nrow=9),show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "def predict_mask_alt(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "    img_npy = np.asarray(img)\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "    softmask = (1-np.array(mask.filter(ImageFilter.GaussianBlur))/255.0)\n",
    "    masked_img = img * (1-softmask) + white_img_npy * (softmask)\n",
    "\n",
    "    return Image.fromarray(masked_img.astype(np.uint8))\n",
    "\n",
    "def predict_mask(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "    img_npy = np.asarray(img)\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "    hardmask = (1-np.array(mask)/255.0)\n",
    "    masked_img = img * (1-hardmask) + white_img_npy * (hardmask)\n",
    "\n",
    "    return Image.fromarray(masked_img.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "B_path = glob.glob('./datasets/getchu_aisacelebaselfie_aligned_v6/trainB/*')\n",
    "tmp_ = None\n",
    "for i in tqdm.tqdm_notebook(range(1000)):\n",
    "    p = random.random()\n",
    "    img_path = B_path[i]\n",
    "#     if p > 0.5:\n",
    "#         img_path = './datasets/getchu_aisacelebaselfie_aligned_cleaned_masked_blur_v6/trainB/'+img_path.split('/')[-1]\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = img\n",
    "    #fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    fake_A_8 = test_img(img_aligned, net_6['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "\n",
    "    show(fake_A_8.data[0].cpu(),show=False).save('../fake_detect/gliches/'+img_path.split('/')[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir ../fake_detect/gliches0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "\n",
    "def predict_mask_alt(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    #print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "    img_npy = np.asarray(img)\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "    softmask = (1-np.array(mask.filter(ImageFilter.GaussianBlur))/255.0)\n",
    "    masked_img = img * (1-softmask) + white_img_npy * (softmask)\n",
    "\n",
    "    return Image.fromarray(masked_img.astype(np.uint8))\n",
    "\n",
    "B_path = glob.glob('../fake_detect/aligned/*')\n",
    "tmp_ = None\n",
    "# for i in tqdm.tqdm_notebook(range(len(B_path))):\n",
    "p = random.random()\n",
    "img_path = random.choice(B_path)\n",
    "# if 'json' in img_path:\n",
    "#     continue\n",
    "#     attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "#     if 'fm1x' not in attr.keys():\n",
    "#         continue\n",
    "img_aligned = img\n",
    "if p > 0.5:\n",
    "    img_aligned = predict_mask_alt(img_aligned)\n",
    "#fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_8 = test_img(img_aligned, net_16['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "show(fake_A_8.data[0].cpu(),show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(glob.glob('../fake_detect/aligned/*')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path = glob.glob('../fake_detect/aligned/*')\n",
    "tmp_ = None\n",
    "for i in tqdm.tqdm_notebook(range(len(B_path))):\n",
    "    p = random.random()\n",
    "    img_path = B_path[i]\n",
    "    if 'json' in img_path:\n",
    "        continue\n",
    "    attr = face_detect(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    img_masked = predict_mask_alt(img_aligned)\n",
    "    #fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    fake_A = test_img(img_aligned, net_16['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    show(fake_A_8.data[0].cpu(),show=False).save('../fake_detect/result_filtered/'+img_path.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path = glob.glob('../fake_detect/random/*')\n",
    "tmp_ = None\n",
    "for i in tqdm.tqdm_notebook(range(len(B_path))):\n",
    "    p = random.random()\n",
    "    img_path = B_path[i]\n",
    "    if 'json' in img_path:\n",
    "        continue\n",
    "    attr = face_detect(img_path)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_aligned = align(img, attr)\n",
    "    if p > 0.5:\n",
    "        img_aligned = predict_mask_alt(img_aligned)\n",
    "    #fake_A_7 = test_img(img_aligned, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    fake_A_8 = test_img(img_aligned, net_16['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "    show(fake_A_8.data[0].cpu(),show=False).save('../fake_detect/result_random/'+img_path.split('/')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = align(Image.open('../face_lib_points/test/nan2.jpg'), attr = face_detect('../face_lib_points/test/nan2.jpg'))\n",
    "tmp2 = Image.open('../face_lib_points/test/nan2.jpg.output.jpg')\n",
    "\n",
    "fake_A_1 = test_img(tmp1, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_2 = test_img(tmp1, net_2['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_3 = test_img(tmp2, net_15['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_4 = test_img(tmp2, net_2['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(tmp1), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), \n",
    "                                        img_pre_128(tmp2), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu()], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = align(Image.open('../face_lib_points/test/nan2.jpg'), attr = face_detect('../face_lib_points/test/nan2.jpg'))\n",
    "tmp2 = Image.open('../face_lib_points/test/nan2.jpg.output.jpg')\n",
    "tmpshow = align_eye_pad_ailab_show(Image.open('../face_lib_points/test/nan2.jpg'), anno=face_detect('../face_lib_points/test/nan2.jpg'), lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0)\n",
    "\n",
    "fake_A_1 = test_img(tmp1, net_11['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_2 = test_img(tmp1, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_3 = test_img(tmp2, net_11['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "fake_A_4 = test_img(tmp2, net_12['B'], img_pre_128, 128, bn_eval=False, drop_eval=True)\n",
    "tmp = show(torchvision.utils.make_grid([img_pre_128(tmpshow), fake_A_1.data[0].cpu(), fake_A_2.data[0].cpu(), \n",
    "                                        img_pre_128(tmpshow), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu()], nrow=3),show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_eye_pad_ailab_show(im, anno, lambda1, lambda2, lambda3):\n",
    "    p1 = np.array((anno['fm1x'], anno['fm1y'])).astype('f')\n",
    "    p2 = np.array((anno['fm0x'], anno['fm0y'])).astype('f')\n",
    "    face_width = anno['y2'] - anno['y1']\n",
    "    angle = angle_between_2_points(p1, p2)\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    xc = (x1 + x2) // 2\n",
    "    yc = (y1 + y2) // 2\n",
    "    dis_width = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2.0\n",
    "    pad_type = 'edge'\n",
    "    pad_size = max(im.size[0], im.size[1]) / 2\n",
    "    np_im = np.array(im)\n",
    "    tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                      np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "    tmp_im = ImageOps.mirror(tmp_im)\n",
    "    xc = xc + pad_size\n",
    "    yc = yc + pad_size\n",
    "    tmp_im = tmp_im.rotate(angle, center=(xc, yc), resample=Image.BICUBIC)\n",
    "    w = face_width\n",
    "    h = w / lambda1 * lambda2\n",
    "    x1 = anno['y1'] - w/2 + pad_size\n",
    "    y1 = yc - w / lambda1 * lambda3\n",
    "    xd = 2*w\n",
    "    yd = h\n",
    "    print xd, yd\n",
    "    return tmp_im.crop((x1-(yd-xd)/2,y1,x1-(yd-xd)/2+yd,y1+yd)).resize((128,128), resample=Image.BICUBIC)\n",
    "\n",
    "tmp = align_eye_pad_ailab_show(Image.open('../face_lib_points/test/nan2.jpg'), anno=face_detect('../face_lib_points/test/nan2.jpg'), lambda1 = 77.0, lambda2 = 228.0, lambda3 = 111.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('../face_lib_points/test/nan2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m = net_12['B']\n",
    "def set_eval(net, bn=True, drop=True):\n",
    "    if isinstance(net, torch.nn.Dropout):\n",
    "        net.training = not drop\n",
    "    elif  isinstance(net, torch.nn.BatchNorm2d) or  isinstance(net, torch.nn.InstanceNorm2d):\n",
    "        net.training = not bn\n",
    "    else:\n",
    "        net.training = False\n",
    "    for i in net.children():\n",
    "        set_eval(i, bn, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_masked = predict_mask(img_aligned)\n",
    "fake_A_3 = test_img(img_masked, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(img_masked, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(img_masked, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(img_masked, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(img_masked, net_9['B'], img_pre_128, 128, eval_mode=True)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img_aligned), img_pre_128(img_masked), fake_A_3.data[0].cpu(), fake_A_4.data[0].cpu(), fake_A_7.data[0].cpu(), fake_A_8.data[0].cpu(), fake_A_9.data[0].cpu()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_p = random.choice(B_path)\n",
    "name = B_p.split('/')[-1]\n",
    "# B_p = os.path.join('../../../clean_dataset/out/', B_p)\n",
    "B_ = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "fake_A_1 = test_img(B_, net_1['B'], img_pre_128, 128,eval_mode=True)\n",
    "fake_A_2 = test_img(B_, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(B_, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(B_, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(B_, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(B_, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_10 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "#show(torchvision.utils.make_grid([img_pre_128(B_),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(),fake_A_5.data[0].cpu(),fake_A_6.data[0].cpu()]),show=False)\n",
    "show(torchvision.utils.make_grid([img_pre_128(B_),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(),fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(),fake_A_9.data[0].cpu()]),show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B_p = random.choice(B_path)\n",
    "#name = B_p.split('/')[-1]\n",
    "# B_p = os.path.join('../../../clean_dataset/out/', B_p)\n",
    "\n",
    "\n",
    "def predict_mask(img):\n",
    "    image_width =  None\n",
    "    image_height = None\n",
    "\n",
    "    image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "    r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "    print r.status_code\n",
    "    #print r.content\n",
    "    js = r.json()\n",
    "    #print js\n",
    "    mask = _get_img(js['prob'], image_width, image_height)\n",
    "    #image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "    img_npy = np.asarray(img)\n",
    "\n",
    "    mask = mask.point(lambda p: p > 50 and 255) \n",
    "    #mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "    mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "    mask_inv_npy = 1 - mask_npy\n",
    "    white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "    img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "    img_masked = Image.fromarray(img_masked_npy)\n",
    "    return img_masked\n",
    "\n",
    "img = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "img_masked = predict_mask(img)\n",
    "fake_A_1 = test_img(img_masked, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_2 = test_img(img_masked, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(img_masked, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(img_masked, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(img_masked, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(img_masked, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(img_masked, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(img_masked, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(img_masked, net_9['B'], img_pre_128, 128, eval_mode=True)\n",
    "#show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(),fake_A_6.data[0].cpu()]),show=False)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(), fake_A_9.data[0].cpu()]),show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def hisEqulColor(img):\n",
    "    img_np = np.array(img)\n",
    "    img_cv = img_np[:,:,::-1]\n",
    "    ycrcb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2YCR_CB)\n",
    "    channels = cv2.split(ycrcb)\n",
    "    cv2.equalizeHist(channels[0], channels[0])\n",
    "    cv2.merge(channels, ycrcb)\n",
    "    img_cv = cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR)\n",
    "    img_np = img_cv[:,:,::-1]\n",
    "    img = Image.fromarray(img_np)\n",
    "    return img\n",
    "\n",
    "B_ = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "B_ = hisEqulColor(B_)\n",
    "\n",
    "fake_A_1 = test_img(B_, net_1['B'], img_pre_128, 128,eval_mode=True)\n",
    "fake_A_2 = test_img(B_, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(B_, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(B_, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(B_, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(B_, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(B_, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_10 = test_img(B_, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "show(torchvision.utils.make_grid([img_pre_128(B_), fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(),fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(),fake_A_9.data[0].cpu()]),show=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def hisEqulColor(img):\n",
    "    img_np = np.array(img)\n",
    "    img_cv = img_np[:,:,::-1]\n",
    "    ycrcb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2YCR_CB)\n",
    "    channels = cv2.split(ycrcb)\n",
    "    cv2.equalizeHist(channels[0], channels[0])\n",
    "    cv2.merge(channels, ycrcb)\n",
    "    img_cv = cv2.cvtColor(ycrcb, cv2.COLOR_YCR_CB2BGR)\n",
    "    img_np = img_cv[:,:,::-1]\n",
    "    img = Image.fromarray(img_np)\n",
    "    return img\n",
    "\n",
    "B_ = Image.open(B_p).convert('RGB').resize((128,128), Image.LANCZOS)#.crop((2,2,126,126))\n",
    "\n",
    "img = hisEqulColor(B_)\n",
    "\n",
    "image_width =  None\n",
    "image_height = None\n",
    "\n",
    "image_base64 = _to_img(img, image_width, image_height)\n",
    "\n",
    "r = requests.post(URL, json={\"session_id\": \"xiaolongzhu\", \"img_data\": image_base64})\n",
    "print r.status_code\n",
    "#print r.content\n",
    "js = r.json()\n",
    "#print js\n",
    "mask = _get_img(js['prob'], image_width, image_height)\n",
    "#image_matting = _get_img(js['img_data'], image_width, image_height)\n",
    "\n",
    "img_npy = np.asarray(img)\n",
    "\n",
    "mask = mask.point(lambda p: p > 50 and 255) \n",
    "#mask = mask.point(lambda p: p < 128 or 0)\n",
    "\n",
    "mask_npy = np.asarray(mask) / 255\n",
    "\n",
    "mask_inv_npy = 1 - mask_npy\n",
    "white_img_npy = np.ones((img_npy.shape),dtype=np.uint8) * 255\n",
    "\n",
    "\n",
    "img_masked_npy = np.multiply(img_npy, mask_npy) + np.multiply(white_img_npy, mask_inv_npy)\n",
    "img_masked = Image.fromarray(img_masked_npy)\n",
    "\n",
    "\n",
    "fake_A_1 = test_img(img_masked, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_2 = test_img(img_masked, net_2['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_3 = test_img(img_masked, net_3['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_4 = test_img(img_masked, net_4['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_5 = test_img(img_masked, net_5['B'], img_pre_128, 128, eval_mode=True)\n",
    "#fake_A_6 = test_img(img_masked, net_6['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_7 = test_img(img_masked, net_7['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_8 = test_img(img_masked, net_8['B'], img_pre_128, 128, eval_mode=True)\n",
    "fake_A_9 = test_img(img_masked, net_9['B'], img_pre_128, 128, eval_mode=True)\n",
    "#show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_5.data[0].cpu(),fake_A_6.data[0].cpu()]),show=False)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img_masked),fake_A_1.data[0].cpu(),fake_A_2.data[0].cpu(),fake_A_3.data[0].cpu(),fake_A_4.data[0].cpu(), fake_A_7.data[0].cpu(),fake_A_8.data[0].cpu(), fake_A_9.data[0].cpu()]),show=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageOps\n",
    "\n",
    "def resize_plot(x=0.37, d=0.87, rot=1.0):\n",
    "    B_ = Image.open(B_p).convert('RGB')#.crop((1445,0,3666,2028))\n",
    "    max_base_width = 512\n",
    "    wpercent = (max_base_width/float(B_.size[0]))\n",
    "    hsize = int((float(B_.size[1])*float(wpercent)))\n",
    "    B_1 = B_.resize((max_base_width,hsize), Image.BICUBIC).rotate(0)\n",
    "    B_2 = B_1\n",
    "    if rot!= 0:\n",
    "        B_2_orig = B_2\n",
    "        np_im = np.array(B_2)\n",
    "        pad_type = 'edge'\n",
    "        pad_size = max(B_2.size[0], B_2.size[1]) / 2\n",
    "        tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "                                  np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "                                  np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "        B_2 = ImageOps.mirror(tmp_im)\n",
    "        B_2 = B_2.rotate(rot, Image.BICUBIC)\n",
    "        B_2 = B_2.crop((pad_size, pad_size, pad_size+B_2_orig.size[0], pad_size+B_2_orig.size[1]))\n",
    "    B_2 = B_2.resize((int((B_1.size[0])*x),int((B_1.size[1])*(x*d))), Image.BICUBIC)\n",
    "#     B_2_pad = Image.new('RGB',(B_2.size[0]+60, B_2.size[1]+60), (0,0,0))\n",
    "#     np_im = np.array(B_2)\n",
    "#     pad_type = 'mean'\n",
    "#     pad_size = 30\n",
    "#     tmp_im = Image.fromarray(np.rot90(np.array([np.pad(np_im[:,:,0], pad_size, pad_type), \\\n",
    "#                               np.pad(np_im[:,:,1], pad_size, pad_type), \\\n",
    "#                               np.pad(np_im[:,:,2], pad_size, pad_type)]).T, 3))\n",
    "#     B_2_pad = ImageOps.mirror(tmp_im)\n",
    "#     B_2 = B_2_pad\n",
    "#     print B_2_pad.size, B_2.size\n",
    "    fake_A_1 = test_img(B_2, net_2['B'], img_id_pre, 128, eval_mode=True)\n",
    "    B_2_show = Image.new('RGB', (fake_A_1.shape[-1], fake_A_1.shape[-2]), (0,0,0))\n",
    "    print B_2.size, B_2_show.size\n",
    "    B_2_show.paste(B_2, (0,0))#B_2.getbbox())\n",
    "    show(torchvision.utils.make_grid([img_id_pre(B_2_show),fake_A_1.data[0].cpu()]))\n",
    "\n",
    "interact(resize_plot, x=(0,1,0.01), d=(0.5,1.2,0.01), rot=(-30,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_plot2(x=0.27, d=0.0):\n",
    "    B_ = Image.open(B_p).convert('RGB').crop((129,0,479,464))\n",
    "    base_width = 512\n",
    "    wpercent = (base_width/float(B_.size[0]))\n",
    "    hsize = int((float(B_.size[1])*float(wpercent)))\n",
    "    B_1 = B_.resize((base_width,hsize), Image.BICUBIC)\n",
    "    B_2 = B_1.resize((int((B_1.size[0])*x),int((B_1.size[1])*(x+d))), Image.ANTIALIAS)\n",
    "    fake_A_1 = test_img(B_2, net_2['B'], img_id_pre, 128, eval_mode=True)\n",
    "    B_2_show = Image.new('RGB', (fake_A_1.shape[-1], fake_A_1.shape[-2]), (0,0,0))\n",
    "    print B_2.size, B_2_show.size\n",
    "    B_2_show.paste(B_2, B_2.getbbox())\n",
    "    #print net_1_d['B'].forward(fake_A_1).mean().data \n",
    "    try:\n",
    "        show(torchvision.utils.make_grid([img_id_pre(B_2_show),fake_A_1.data[0].cpu()]))\n",
    "    except Exception:\n",
    "        show(torchvision.utils.make_grid([img_id_pre(B_2_show)]))\n",
    "        show(torchvision.utils.make_grid([fake_A_1.data[0].cpu()]))\n",
    "\n",
    "interact(resize_plot2, x=(0,1,0.01), d=(-0.5,0.5,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1 = get_nets('./checkpoints/Get_AisCel_128_4_aff_1_resize_caffe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = '/data2/minjunli/prj/anime/face_lib_points/test/testimg/test.png'\n",
    "attr = face_detect(img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img_aligned = align(img, attr)\n",
    "\n",
    "fake_A_1 = test_img(img_aligned, net_1['B'], img_pre_128, 128, eval_mode=True)\n",
    "show(torchvision.utils.make_grid([img_pre_128(img), img_pre_128(img_aligned), fake_A_1.data[0].cpu()]),show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb\n",
    "import pdb\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import pytorch2caffe\n",
    "\n",
    "m = net_1['B'].cpu()\n",
    "m.eval()\n",
    "print(m)\n",
    "\n",
    "input_var = Variable(torch.rand(1, 3, 128, 128))\n",
    "output_var = m(input_var)\n",
    "\n",
    "output_dir = 'caffe'\n",
    "pytorch2caffe.pytorch2caffe(input_var, output_var, \n",
    "              os.path.join(output_dir, 'trans.prototxt'),\n",
    "              os.path.join(output_dir, 'trans.caffemodel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(pytorch2caffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data2/minjunli/tol/caffe/python/\")\n",
    "sys.path.append(\"/data2/minjunli/tol/pytorch2caffe/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb\n",
    "import pdb\n",
    "pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = caffe.Net('./caffe/trans.prototxt', caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "7e2eb5fb0ab742be98b936bdeadf584c": {
     "views": [
      {
       "cell_index": 79
      }
     ]
    },
    "adc70966e3464c72b2e0d1d1291b64b2": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "cb4d1b442e6942498c4cec7507d090d3": {
     "views": [
      {
       "cell_index": 68
      }
     ]
    },
    "d039635de9544c9e8343be22ad2d2076": {
     "views": [
      {
       "cell_index": 83
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
